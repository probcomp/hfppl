{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>LLaMPPL is a research prototype for language model probabilistic programming: specifying language generation tasks by writing probabilistic programs that combine calls to LLMs, symbolic program logic, and probabilistic conditioning. To solve these tasks, LLaMPPL uses a specialized sequential Monte Carlo inference algorithm.</p> <p>This technique, SMC steering, is described in our workshop abstract, Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs.</p>"},{"location":"anatomy/","title":"Anatomy of a LLaMPPL model","text":""},{"location":"batching/","title":"Auto-Batching","text":"<p>If running in a GPU-accelerated environment, LLaMPPL supports auto-batching.</p> <p>The <code>step</code> method of a LLaMPPL model describes how to advance a single particle one step of generation. But inference methods must maintain many particles at once.</p> <p>With auto-batching, LLaMPPL will execute particles' <code>step</code> methods concurrently, and automatically batch calls to large language models. This batching is handled by the <code>CachedCausalLM</code> object, and its behavior is controlled by two parameters:</p> <ul> <li><code>lm.batch_size</code>: the maximum number of requests to batch. The default value is 20.</li> <li><code>lm.timeout</code>: if <code>lm.timeout</code> seconds pass with no new request, the current batch is processed even if not full. The default value is 0.02.</li> </ul> <p>You may want to set the batch size (<code>lm.batch_size</code>) to the number of particles you are using (if the number of particles is not too large).</p>"},{"location":"caching/","title":"Caching in LLaMPPL","text":"<p>LLaMPPL performs two kinds of caching to improve performance.</p>"},{"location":"caching/#log-probability-caching","title":"Log probability caching","text":"<p>Next-token log probabilities are always cached, whenever they are computed. This way, if different particles make exactly the same log probability queries, the Transformer is run only once. This is primarily beneficial when:</p> <ul> <li> <p>particles are cloned during resampling: when each particle is</p> </li> <li> <p>cloned particles happen to sample the same next token: if the next-token distribution is concentrated,   it is likely that multiple copies of a particle will sample the same next token. Log probability caching   allows them to sample the following token using only a single call to the language model.</p> </li> </ul> <p>The log probability cache can be cleared using the <code>lm.clear_cache()</code> method. Note that this method will also clear the KV cache.</p>"},{"location":"caching/#key-value-caching","title":"Key-value caching","text":"<p>Key-value caching caches the key and value vectors computed by each layer of a Transformer, for reuse when processing new tokens at the end of a previously evaluated sequence.</p> <p>In principle, key-value caching is most useful when:</p> <ul> <li> <p>There is a long common prompt from which all particles are generating.   In this case, the prompt's tokens can be evaluated just once by the language model,   and each subsequent call only has to pay for the new tokens generated after the prompt.</p> </li> <li> <p>Generations from the model are very long. In this case, it may be worth paying the memory   cost to cache different key-value sequences for each particle, to speed up future next-token   queries.</p> </li> </ul> <p>Currently, only the first use case is well-supported by the LLaMPPL library, via the <code>lm.cache_kv(prompt)</code> method. This method computes and caches key and value vectors for every token in <code>prompt</code>. Future calls to <code>lm.next_token_logprobs</code> and <code>lm.next_token_logprobs_unbatched</code> will automatically recognize when <code>prompt</code> is a prefix of the new query, and automatically exploit incremental computation. Multiple prompts can be cached, and <code>lm.clear_kv_cache()</code> can be used to clear the KV-cache without clearing the log probability cache.</p> <p>Because <code>lm.cache_kv</code> is not a batched call, it is not well-suited to caching different strings for different particles. Rather, it is best used in the <code>__init__</code> method of a model--or even outside of a model--on fixed prompt strings that every particle will share.</p>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#colab","title":"Colab","text":"<p>One easy way to try LLaMPPL out is to use a Colab notebook. We have a demo notebook that performs constrained generation with GPT-2, a small enough model that the RAM and GPU constraints of Colab's free version should not prevent you from running the demo.</p>"},{"location":"getting_started/#installing-llamppl","title":"Installing LLaMPPL","text":"<p>To get started, clone the <code>hfppl</code> repository and install the <code>hfppl</code> package.</p> <pre><code>git clone https://github.com/probcomp/hfppl\ncd hfppl\npoetry install\n</code></pre> <p>We use poetry to manage dependencies. If you don't have poetry installed, you can install it with <code>pip install poetry</code>.</p> <p>You can then run an example. The first time you run it, the example may ask to downlaod model weights from the HuggingFace model repository.</p> <pre><code>poetry run python examples/hard_constraints.py\n</code></pre> <p>Depending on your available GPU memory, you may wish to edit the example to change parameters such as the batch size, or which HuggingFace model to use. The <code>hard_constraints.py</code> example has been run successfully on an NVIDIA L4 GPU (with 24 GB of VRAM) on Google Cloud.</p>"},{"location":"getting_started/#your-first-model","title":"Your First Model","text":"<p>Let's write a LLaMPPL model to generate according to the hard constraint that completions do not use the lowercase letter <code>e</code>.</p> <p>To do so, we write subclass the <code>Model</code> class:</p> <pre><code># examples/no_e.py\n\nfrom hfppl import Model, LMContext, CachedCausalLM\n\n# A LLaMPPL model subclasses the Model class\nclass MyModel(Model):\n\n    # The __init__ method is used to process arguments\n    # and initialize instance variables.\n    def __init__(self, lm, prompt, forbidden_letter):\n        super().__init__()\n\n        # A stateful context object for the LLM, initialized with the prompt\n        self.context = LMContext(lm, prompt)\n        self.eos_token = lm.tokenizer.eos_token_id\n\n        # The forbidden letter\n        self.forbidden_tokens = set(i for (i, v) in enumerate(lm.vocab)\n                                      if forbidden_letter in v)\n\n    # The step method is used to perform a single 'step' of generation.\n    # This might be a single token, a single phrase, or any other division.\n    # Here, we generate one token at a time.\n    async def step(self):\n        # Condition on the next token *not* being a forbidden token.\n        await self.observe(self.context.mask_dist(self.forbidden_tokens), False)\n\n        # Sample the next token from the LLM -- automatically extends `self.context`.\n        token = await self.sample(self.context.next_token())\n\n        # Check for EOS or end of sentence\n        if token.token_id == self.eos_token or str(token) in ['.', '!', '?']:\n            # Finish generation\n            self.finish()\n\n    # To improve performance, a hint that `self.forbidden_tokens` is immutable\n    def immutable_properties(self):\n        return set(['forbidden_tokens'])\n</code></pre> <p>To run the model, we use an inference method, like <code>smc_steer</code>:</p> <pre><code>import asyncio\nfrom hfppl import smc_steer\n\n# Initialize the HuggingFace model\nlm = CachedCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", auth_token=&lt;YOUR_HUGGINGFACE_API_TOKEN_HERE&gt;)\n\n# Create a model instance\nmodel = MyModel(lm, \"The weather today is expected to be\", \"e\")\n\n# Run inference\nparticles = asyncio.run(smc_steer(model, 5, 3)) # number of particles N, and beam factor K\n</code></pre> <p>Each returned particle is an instance of the <code>MyModel</code> class that has been <code>step</code>-ped to completion. The generated strings can be printed along with the particle weights:</p> <pre><code>for particle in particles:\n    print(f\"{particle.context.s} (weight: {particle.weight})\")\n</code></pre>"},{"location":"getting_started/#learning-more","title":"Learning more","text":"<p>For more intuition on language model probabilistic programming, see our paper, or the rest of this documentation.</p>"},{"location":"immutability/","title":"Immutability","text":"<p>When a particle is promising, the sequential Monte Carlo algorithm may clone it, by calling <code>copy.deepcopy</code>.</p> <p>Depending on your model, this may be more or less expensive.</p> <p>To make it faster, override the <code>immutable_properties(self)</code> method of your Model class, to return a <code>set[str]</code> of property names that are guaranteed not to change during <code>step</code>. For all properties in this set, LLaMPPL will use shared memory across particles, and avoid copying when cloning particles.</p>"},{"location":"performance/","title":"Improving performance of LLaMPPL models","text":"<p>If your LLaMPPL model is running slowly, consider exploiting the following features to improve performance:</p> <ul> <li>Auto-Batching \u2014 to run multiple particles concurrently, with batched LLM calls</li> <li>Caching - to cache key and value vectors for long prompts</li> <li>Immutability hinting - to significantly speed up the bookkeeping performed by SMC inference</li> </ul>"},{"location":"transformers/","title":"Working with Transformers","text":""},{"location":"transformers/#load-your-transformer-as-a-cachedcausallm","title":"Load your Transformer as a <code>CachedCausalLM</code>","text":"<p>The easiest way to load a Transformer model is to use the <code>CachedCausalLM.from_pretrained</code> static method, which accepts as input a HuggingFace model identifier. This loads the model's weights into memory, and also loads the appropriate tokenizer. The optional <code>auth_token</code> parameter can be provided if the model in question requires HuggingFace authorization (e.g., Meta's Llama 2 models).</p>"},{"location":"transformers/#use-the-llm-within-your-model-via-the-transformer-distribution","title":"Use the LLM within your model via the <code>Transformer</code> distribution","text":"<p>Within a model, you can <code>sample</code> or <code>observe</code> from the <code>Transformer</code> distribution. It accepts as arguments a <code>CachedCausalLM</code> instance, as well as a list of integer token ids specifying the context. It returns a distribution over next tokens. The <code>Transformer</code> distirbution is stateless, and so your model will need to manually extend the context with newly sampled tokens.</p>"},{"location":"transformers/#use-the-llm-within-your-model-via-the-lmcontext-class","title":"Use the LLM within your model via the <code>LMContext</code> class","text":"<p>Alternatively, you can initialize an <code>LMContext</code> object with a <code>CachedCausalLM</code> instance instance and a string-valued prompt. It maintains a growing context as state, and exposes a <code>next_token</code> distribution that, when sampled, observed, or intervened, grows the context. It also supports a form of 'sub-token' generation, via the <code>mask_dist</code> distribution.</p>"},{"location":"transformers/#create-custom-token-distributions-with-tokencategorical","title":"Create custom token distributions with <code>TokenCategorical</code>","text":"<p>You may also create a custom distribution over the vocabulary of a language model using the <code>TokenCategorical</code> distribution. It is parameterized by a <code>CachedCausalLM</code> instance, and an array of logits equal in length to the language model's vocabulary size. This distribution is particularly useful as a proposal distribution; for example, a model might <code>sample</code> with <code>dist</code> set to the LM's next token distribution, but with <code>proposal</code> set to a modified distribution that uses a heuristic to upweight 'good' tokens and downweight 'bad' ones.</p>"},{"location":"visualization/","title":"Visualization","text":"<p>We provide a Web interface for visualizing the execution of a sequential Monte Carlo algorithm, based on contributions from Maddy Bowers and Jacob Hoover.</p> <p>First, update your model to support visualization by implementing the <code>string_for_serialization</code> method. Return a string that summarizes the particle's current state.</p> <p>To run the interface, change to the <code>html</code> directory and run <code>python -m http.server</code>. This will start serving the files in the <code>html</code> directory at localhost:8000. (If you are SSH-ing onto a remote machine, you may need port forwarding. Visual Studio Code automatically handles this for some ports, including 8000.) Then, when calling <code>smc_standard</code>, set <code>visualization_dir</code> to the path to the <code>html</code> directory. A JSON record of the run will automatically be saved to that directory, and a URL will be printed to the console (<code>http://localhost:8000/smc.html?path=$json_file</code>).</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>hfppl<ul> <li>chunks</li> <li>distributions<ul> <li>bernoulli</li> <li>distribution</li> <li>geometric</li> <li>lmcontext</li> <li>logcategorical</li> <li>tokencategorical</li> <li>transformer</li> </ul> </li> <li>inference<ul> <li>smc_record</li> <li>smc_standard</li> <li>smc_steer</li> </ul> </li> <li>llms</li> <li>modeling</li> <li>util</li> </ul> </li> </ul>"},{"location":"reference/hfppl/__init__/","title":"hfppl","text":"<p>Probabilistic programming with HuggingFace Transformer models.</p>"},{"location":"reference/hfppl/__init__/#hfppl.Bernoulli","title":"<code>Bernoulli</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A Bernoulli distribution.</p> Source code in <code>hfppl/distributions/bernoulli.py</code> <pre><code>class Bernoulli(Distribution):\n    \"\"\"A Bernoulli distribution.\"\"\"\n\n    def __init__(self, p):\n        \"\"\"Create a Bernoulli distribution.\n\n        Args:\n            p: the probability-of-True for the Bernoulli distribution.\n        \"\"\"\n        self.p = p\n\n    async def sample(self):\n        b = np.random.rand() &lt; self.p\n        return (b, await self.log_prob(b))\n\n    async def log_prob(self, value):\n        return np.log(self.p) if value else np.log1p(-self.p)\n\n    async def argmax(self, idx):\n        return (self.p &gt; 0.5) if idx == 0 else (self.p &lt; 0.5)\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Bernoulli.__init__","title":"<code>__init__(p)</code>","text":"<p>Create a Bernoulli distribution.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <p>the probability-of-True for the Bernoulli distribution.</p> required Source code in <code>hfppl/distributions/bernoulli.py</code> <pre><code>def __init__(self, p):\n    \"\"\"Create a Bernoulli distribution.\n\n    Args:\n        p: the probability-of-True for the Bernoulli distribution.\n    \"\"\"\n    self.p = p\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.CachedCausalLM","title":"<code>CachedCausalLM</code>","text":"<p>Wrapper around a <code>genlm_backend.llm.AsyncLM</code>.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>AsyncLM</code> <p>The underlying language model (either <code>AsyncVirtualLM</code> or <code>AsyncTransformer</code>).</p> <code>str_vocab</code> <code>list[str]</code> <p>List mapping token IDs to their string representations.</p> <code>byte_vocab</code> <code>list[bytes]</code> <p>List mapping token IDs to their byte representations.</p> <code>masks</code> <code>Masks</code> <p>Token masks for filtering logits during generation.</p> Source code in <code>hfppl/llms.py</code> <pre><code>class CachedCausalLM:\n    \"\"\"Wrapper around a [`genlm_backend.llm.AsyncLM`](https://probcomp.github.io/genlm-backend/reference/genlm_backend/llm/__init__/).\n\n    Attributes:\n        model (genlm_backend.llm.AsyncLM): The underlying language model (either `AsyncVirtualLM` or `AsyncTransformer`).\n        str_vocab (list[str]): List mapping token IDs to their string representations.\n        byte_vocab (list[bytes]): List mapping token IDs to their byte representations.\n        masks (Masks): Token masks for filtering logits during generation.\n    \"\"\"\n\n    @classmethod\n    def from_pretrained(cls, model_id, backend=None, **kwargs):\n        \"\"\"Create a CachedCausalLM from a HuggingFace model name.\n\n        This is a convenience method that instantiates the underlying `AsyncLM` from a HuggingFace model name.\n\n        Args:\n            model_id (str): Name or path of the HuggingFace pretrained model to load.\n            backend (str, optional): `AsyncLM` backend to use:\n                - 'vllm' to instantiate an `AsyncVirtualLM`; ideal for GPU usage\n                - 'hf' for an `AsyncTransformer`; ideal for CPU usage\n                - 'mock' for a `MockAsyncLM`; ideal for testing.\n                Defaults to 'vllm' if CUDA is available, otherwise 'hf'.\n            **kwargs: Additional keyword arguments passed to the `AsyncLM` constructor.\n                See [`AsyncLM` documentation](https://probcomp.github.io/genlm-backend/reference/genlm_backend/llm/__init__/).\n\n        Returns:\n            CachedCausalLM: The hfppl-compatible interface to the `AsyncLM` model.\n        \"\"\"\n        backend = backend or (\n            \"vllm\" if (torch.cuda.is_available() and VLLM_AVAILABLE) else \"hf\"\n        )\n\n        if backend == \"vllm\":\n            if not VLLM_AVAILABLE:\n                raise ValueError(\n                    \"vLLM backend requested but vLLM is not installed. \"\n                    \"Please install vLLM with `pip install vllm`.\"\n                )\n            model_cls = AsyncVirtualLM\n        elif backend == \"hf\":\n            model_cls = AsyncTransformer\n        elif backend == \"mock\":\n            model_cls = MockAsyncLM\n        else:\n            raise ValueError(\n                f\"Unknown backend: {backend}. Must be one of ['vllm', 'hf', 'mock']\"\n            )\n\n        # Handle legacy auth_token parameter. The ability to pass in the auth_token should\n        # be removed in a future version since it is not supported by the vllm backend.\n        # Users should authenticate with the HuggingFace CLI.\n        auth_token = kwargs.pop(\"auth_token\", None)\n        if auth_token:\n            if backend == \"vllm\":\n                raise ValueError(\n                    \"Explicitly passing auth_token is not compatible with the vLLM AsyncLM backend. \"\n                    \"Authenticate using `huggingface-cli login` instead.\"\n                )\n\n            if \"hf_opts\" not in kwargs:\n                kwargs[\"hf_opts\"] = {}\n            kwargs[\"hf_opts\"][\"token\"] = auth_token\n\n            warnings.warn(\n                \"Passing auth_token directly is deprecated and will be removed in a future version. \"\n                \"Please authenticate using `huggingface-cli login` instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        load_in_8bit = kwargs.pop(\"load_in_8bit\", False)\n        if load_in_8bit:\n            if \"bitsandbytes_opts\" not in kwargs:\n                kwargs[\"bitsandbytes_opts\"] = {}\n            kwargs[\"bitsandbytes_opts\"][\"load_in_8bit\"] = True\n\n            warnings.warn(\n                \"load_in_8bit is deprecated and will be removed in a future version. \"\n                \"Please pass `bitsandbytes_opts` instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        model = model_cls.from_name(model_id, **kwargs)\n\n        return cls(model)\n\n    def __init__(self, model):\n        \"\"\"\n        Create a `CachedCausalLM` from an `AsyncLM`.\n\n        Args:\n            model (genlm_backend.llm.AsyncLM): an `AsyncLM` instance.\n        \"\"\"\n        if isinstance(model, AsyncVirtualLM):\n            self.backend = \"vllm\"\n        elif isinstance(model, AsyncTransformer):\n            self.backend = \"hf\"\n        elif isinstance(model, MockAsyncLM):\n            self.backend = \"mock\"\n        else:\n            raise ValueError(\n                f\"Unknown model type: {type(model)}. Must be one of [AsyncVirtualLM, AsyncTransformer, MockAsyncLM]\"\n            )\n\n        self.model = model\n        self.tokenizer = model.tokenizer\n        self.str_vocab = model.str_vocab\n        self.byte_vocab = model.byte_vocab\n        self.masks = Masks(self)\n\n    @property\n    def vocab(self):\n        \"\"\"Legacy accessor for string vocabulary. Prefer using `.str_vocab` directly for access to the model's string vocabulary.\"\"\"\n        warnings.warn(\n            \"Accessing .vocab directly is deprecated and will be removed in a future version. Use .str_vocab or .byte_vocab instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.model.str_vocab\n\n    def __deepcopy__(self, memo):\n        return self\n\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token. This version is asynchronous and support auto batching of concurrent requests; use with `await`.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (numpy.array): a numpy array of length `len(str_vocab)` (equivalently `len(byte_vocab)`) with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        logprobs = await self.model.next_token_logprobs(token_ids)\n        return logprobs.float().cpu().numpy()\n\n    def next_token_logprobs_unbatched(self, token_ids):\n        \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (numpy.array): a numpy array of length `len(str_vocab)` (equivalently `len(byte_vocab)`) with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        return self.model.next_token_logprobs_sync(token_ids).float().cpu().numpy()\n\n    def clear_cache(self):\n        \"\"\"Clear the cache of log probabilities and key/value pairs.\n\n        For HuggingFace backend: Clears both logprob cache and KV cache.\n\n        For vLLM backend: Only clears logprob cache (KV cache is managed internally by vLLM).\n        \"\"\"\n        self.model.clear_cache()\n\n    def clear_kv_cache(self):\n        \"\"\"Clear any key and value vectors from the cache.\"\"\"\n        if self.backend == \"hf\":\n            self.model.clear_kv_cache()\n        elif self.backend == \"vllm\":\n            warnings.warn(\n                \"clear_kv_cache() is only supported for the HuggingFace backend. The KV cache for the vLLM backend is handled internally by vLLM. No operation performed.\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        elif self.backend == \"mock\":\n            pass\n        else:\n            raise RuntimeError(\n                f\"clear_kv_cache() is not implemented for backend type {type(self.model)}\"\n            )\n\n    def reset_async_queries(self):\n        \"\"\"Clear any pending language model queries from the queue.\"\"\"\n        if self.backend == \"hf\":\n            self.model.reset_async_queries()\n        elif self.backend == \"vllm\":\n            warnings.warn(\n                \"reset_async_queries() is only supported for the HuggingFace backend. No operation performed.\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        elif self.backend == \"mock\":\n            pass\n        else:\n            raise RuntimeError(\n                f\"reset_async_queries() is not implemented for backend type {type(self.model)}\"\n            )\n\n    def cache_kv(self, prompt_tokens):\n        \"\"\"Cache the key and value vectors for a prompt.\n\n        Args:\n            prompt_tokens (list[int]): token ids for the prompt to cache.\n        \"\"\"\n        if self.backend == \"hf\":\n            self.model.cache_kv(prompt_tokens)\n        elif self.backend == \"vllm\":\n            warnings.warn(\n                \"cache_kv() is only supported for the HuggingFace backend. The KV cache for the vLLM backend is handled internally by vLLM. No operation performed.\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        elif self.backend == \"mock\":\n            pass\n        else:\n            raise RuntimeError(\n                f\"cache_kv() is not implemented for backend type {type(self.model)}\"\n            )\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.CachedCausalLM.vocab","title":"<code>vocab</code>  <code>property</code>","text":"<p>Legacy accessor for string vocabulary. Prefer using <code>.str_vocab</code> directly for access to the model's string vocabulary.</p>"},{"location":"reference/hfppl/__init__/#hfppl.CachedCausalLM.__init__","title":"<code>__init__(model)</code>","text":"<p>Create a <code>CachedCausalLM</code> from an <code>AsyncLM</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>AsyncLM</code> <p>an <code>AsyncLM</code> instance.</p> required Source code in <code>hfppl/llms.py</code> <pre><code>def __init__(self, model):\n    \"\"\"\n    Create a `CachedCausalLM` from an `AsyncLM`.\n\n    Args:\n        model (genlm_backend.llm.AsyncLM): an `AsyncLM` instance.\n    \"\"\"\n    if isinstance(model, AsyncVirtualLM):\n        self.backend = \"vllm\"\n    elif isinstance(model, AsyncTransformer):\n        self.backend = \"hf\"\n    elif isinstance(model, MockAsyncLM):\n        self.backend = \"mock\"\n    else:\n        raise ValueError(\n            f\"Unknown model type: {type(model)}. Must be one of [AsyncVirtualLM, AsyncTransformer, MockAsyncLM]\"\n        )\n\n    self.model = model\n    self.tokenizer = model.tokenizer\n    self.str_vocab = model.str_vocab\n    self.byte_vocab = model.byte_vocab\n    self.masks = Masks(self)\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.CachedCausalLM.cache_kv","title":"<code>cache_kv(prompt_tokens)</code>","text":"<p>Cache the key and value vectors for a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_tokens</code> <code>list[int]</code> <p>token ids for the prompt to cache.</p> required Source code in <code>hfppl/llms.py</code> <pre><code>def cache_kv(self, prompt_tokens):\n    \"\"\"Cache the key and value vectors for a prompt.\n\n    Args:\n        prompt_tokens (list[int]): token ids for the prompt to cache.\n    \"\"\"\n    if self.backend == \"hf\":\n        self.model.cache_kv(prompt_tokens)\n    elif self.backend == \"vllm\":\n        warnings.warn(\n            \"cache_kv() is only supported for the HuggingFace backend. The KV cache for the vLLM backend is handled internally by vLLM. No operation performed.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n    elif self.backend == \"mock\":\n        pass\n    else:\n        raise RuntimeError(\n            f\"cache_kv() is not implemented for backend type {type(self.model)}\"\n        )\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.CachedCausalLM.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the cache of log probabilities and key/value pairs.</p> <p>For HuggingFace backend: Clears both logprob cache and KV cache.</p> <p>For vLLM backend: Only clears logprob cache (KV cache is managed internally by vLLM).</p> Source code in <code>hfppl/llms.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the cache of log probabilities and key/value pairs.\n\n    For HuggingFace backend: Clears both logprob cache and KV cache.\n\n    For vLLM backend: Only clears logprob cache (KV cache is managed internally by vLLM).\n    \"\"\"\n    self.model.clear_cache()\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.CachedCausalLM.clear_kv_cache","title":"<code>clear_kv_cache()</code>","text":"<p>Clear any key and value vectors from the cache.</p> Source code in <code>hfppl/llms.py</code> <pre><code>def clear_kv_cache(self):\n    \"\"\"Clear any key and value vectors from the cache.\"\"\"\n    if self.backend == \"hf\":\n        self.model.clear_kv_cache()\n    elif self.backend == \"vllm\":\n        warnings.warn(\n            \"clear_kv_cache() is only supported for the HuggingFace backend. The KV cache for the vLLM backend is handled internally by vLLM. No operation performed.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n    elif self.backend == \"mock\":\n        pass\n    else:\n        raise RuntimeError(\n            f\"clear_kv_cache() is not implemented for backend type {type(self.model)}\"\n        )\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.CachedCausalLM.from_pretrained","title":"<code>from_pretrained(model_id, backend=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a CachedCausalLM from a HuggingFace model name.</p> <p>This is a convenience method that instantiates the underlying <code>AsyncLM</code> from a HuggingFace model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Name or path of the HuggingFace pretrained model to load.</p> required <code>backend</code> <code>str</code> <p><code>AsyncLM</code> backend to use: - 'vllm' to instantiate an <code>AsyncVirtualLM</code>; ideal for GPU usage - 'hf' for an <code>AsyncTransformer</code>; ideal for CPU usage - 'mock' for a <code>MockAsyncLM</code>; ideal for testing. Defaults to 'vllm' if CUDA is available, otherwise 'hf'.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>AsyncLM</code> constructor. See <code>AsyncLM</code> documentation.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>CachedCausalLM</code> <p>The hfppl-compatible interface to the <code>AsyncLM</code> model.</p> Source code in <code>hfppl/llms.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, model_id, backend=None, **kwargs):\n    \"\"\"Create a CachedCausalLM from a HuggingFace model name.\n\n    This is a convenience method that instantiates the underlying `AsyncLM` from a HuggingFace model name.\n\n    Args:\n        model_id (str): Name or path of the HuggingFace pretrained model to load.\n        backend (str, optional): `AsyncLM` backend to use:\n            - 'vllm' to instantiate an `AsyncVirtualLM`; ideal for GPU usage\n            - 'hf' for an `AsyncTransformer`; ideal for CPU usage\n            - 'mock' for a `MockAsyncLM`; ideal for testing.\n            Defaults to 'vllm' if CUDA is available, otherwise 'hf'.\n        **kwargs: Additional keyword arguments passed to the `AsyncLM` constructor.\n            See [`AsyncLM` documentation](https://probcomp.github.io/genlm-backend/reference/genlm_backend/llm/__init__/).\n\n    Returns:\n        CachedCausalLM: The hfppl-compatible interface to the `AsyncLM` model.\n    \"\"\"\n    backend = backend or (\n        \"vllm\" if (torch.cuda.is_available() and VLLM_AVAILABLE) else \"hf\"\n    )\n\n    if backend == \"vllm\":\n        if not VLLM_AVAILABLE:\n            raise ValueError(\n                \"vLLM backend requested but vLLM is not installed. \"\n                \"Please install vLLM with `pip install vllm`.\"\n            )\n        model_cls = AsyncVirtualLM\n    elif backend == \"hf\":\n        model_cls = AsyncTransformer\n    elif backend == \"mock\":\n        model_cls = MockAsyncLM\n    else:\n        raise ValueError(\n            f\"Unknown backend: {backend}. Must be one of ['vllm', 'hf', 'mock']\"\n        )\n\n    # Handle legacy auth_token parameter. The ability to pass in the auth_token should\n    # be removed in a future version since it is not supported by the vllm backend.\n    # Users should authenticate with the HuggingFace CLI.\n    auth_token = kwargs.pop(\"auth_token\", None)\n    if auth_token:\n        if backend == \"vllm\":\n            raise ValueError(\n                \"Explicitly passing auth_token is not compatible with the vLLM AsyncLM backend. \"\n                \"Authenticate using `huggingface-cli login` instead.\"\n            )\n\n        if \"hf_opts\" not in kwargs:\n            kwargs[\"hf_opts\"] = {}\n        kwargs[\"hf_opts\"][\"token\"] = auth_token\n\n        warnings.warn(\n            \"Passing auth_token directly is deprecated and will be removed in a future version. \"\n            \"Please authenticate using `huggingface-cli login` instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    load_in_8bit = kwargs.pop(\"load_in_8bit\", False)\n    if load_in_8bit:\n        if \"bitsandbytes_opts\" not in kwargs:\n            kwargs[\"bitsandbytes_opts\"] = {}\n        kwargs[\"bitsandbytes_opts\"][\"load_in_8bit\"] = True\n\n        warnings.warn(\n            \"load_in_8bit is deprecated and will be removed in a future version. \"\n            \"Please pass `bitsandbytes_opts` instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    model = model_cls.from_name(model_id, **kwargs)\n\n    return cls(model)\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.CachedCausalLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token. This version is asynchronous and support auto batching of concurrent requests; use with <code>await</code>.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>array</code> <p>a numpy array of length <code>len(str_vocab)</code> (equivalently <code>len(byte_vocab)</code>) with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>hfppl/llms.py</code> <pre><code>async def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token. This version is asynchronous and support auto batching of concurrent requests; use with `await`.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (numpy.array): a numpy array of length `len(str_vocab)` (equivalently `len(byte_vocab)`) with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    logprobs = await self.model.next_token_logprobs(token_ids)\n    return logprobs.float().cpu().numpy()\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.CachedCausalLM.next_token_logprobs_unbatched","title":"<code>next_token_logprobs_unbatched(token_ids)</code>","text":"<p>Request log probabilities of next token. Not asynchronous, and does not support auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>array</code> <p>a numpy array of length <code>len(str_vocab)</code> (equivalently <code>len(byte_vocab)</code>) with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>hfppl/llms.py</code> <pre><code>def next_token_logprobs_unbatched(self, token_ids):\n    \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (numpy.array): a numpy array of length `len(str_vocab)` (equivalently `len(byte_vocab)`) with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    return self.model.next_token_logprobs_sync(token_ids).float().cpu().numpy()\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.CachedCausalLM.reset_async_queries","title":"<code>reset_async_queries()</code>","text":"<p>Clear any pending language model queries from the queue.</p> Source code in <code>hfppl/llms.py</code> <pre><code>def reset_async_queries(self):\n    \"\"\"Clear any pending language model queries from the queue.\"\"\"\n    if self.backend == \"hf\":\n        self.model.reset_async_queries()\n    elif self.backend == \"vllm\":\n        warnings.warn(\n            \"reset_async_queries() is only supported for the HuggingFace backend. No operation performed.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n    elif self.backend == \"mock\":\n        pass\n    else:\n        raise RuntimeError(\n            f\"reset_async_queries() is not implemented for backend type {type(self.model)}\"\n        )\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Distribution","title":"<code>Distribution</code>","text":"<p>Abstract base class for a distribution.</p> Source code in <code>hfppl/distributions/distribution.py</code> <pre><code>class Distribution:\n    \"\"\"Abstract base class for a distribution.\"\"\"\n\n    async def sample(self):\n        \"\"\"Generate a random sample from the distribution.\n\n        Returns:\n            x: a value randomly sampled from the distribution.\"\"\"\n        raise NotImplementedError()\n\n    async def log_prob(self, x):\n        \"\"\"Compute the log probability of a value under this distribution,\n        or the log probability density if the distribution is continuous.\n\n        Args:\n            x: the point at which to evaluate the log probability.\n        Returns:\n            logprob (float): the log probability of `x`.\"\"\"\n        raise NotImplementedError()\n\n    async def argmax(self, n):\n        \"\"\"Return the nth most probable outcome under this distribution (assuming this is a discrete distribution).\n\n        Args:\n            n (int): which value to return to, indexed from most probable (n=0) to least probable (n=|support|).\n        Returns:\n            x: the nth most probable outcome from this distribution.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Distribution.argmax","title":"<code>argmax(n)</code>  <code>async</code>","text":"<p>Return the nth most probable outcome under this distribution (assuming this is a discrete distribution).</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>which value to return to, indexed from most probable (n=0) to least probable (n=|support|).</p> required <p>Returns:     x: the nth most probable outcome from this distribution.</p> Source code in <code>hfppl/distributions/distribution.py</code> <pre><code>async def argmax(self, n):\n    \"\"\"Return the nth most probable outcome under this distribution (assuming this is a discrete distribution).\n\n    Args:\n        n (int): which value to return to, indexed from most probable (n=0) to least probable (n=|support|).\n    Returns:\n        x: the nth most probable outcome from this distribution.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Distribution.log_prob","title":"<code>log_prob(x)</code>  <code>async</code>","text":"<p>Compute the log probability of a value under this distribution, or the log probability density if the distribution is continuous.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>the point at which to evaluate the log probability.</p> required <p>Returns:     logprob (float): the log probability of <code>x</code>.</p> Source code in <code>hfppl/distributions/distribution.py</code> <pre><code>async def log_prob(self, x):\n    \"\"\"Compute the log probability of a value under this distribution,\n    or the log probability density if the distribution is continuous.\n\n    Args:\n        x: the point at which to evaluate the log probability.\n    Returns:\n        logprob (float): the log probability of `x`.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Distribution.sample","title":"<code>sample()</code>  <code>async</code>","text":"<p>Generate a random sample from the distribution.</p> <p>Returns:</p> Name Type Description <code>x</code> <p>a value randomly sampled from the distribution.</p> Source code in <code>hfppl/distributions/distribution.py</code> <pre><code>async def sample(self):\n    \"\"\"Generate a random sample from the distribution.\n\n    Returns:\n        x: a value randomly sampled from the distribution.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Geometric","title":"<code>Geometric</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A Geometric distribution.</p> Source code in <code>hfppl/distributions/geometric.py</code> <pre><code>class Geometric(Distribution):\n    \"\"\"A Geometric distribution.\"\"\"\n\n    def __init__(self, p):\n        \"\"\"Create a Geometric distribution.\n\n        Args:\n            p: the rate of the Geometric distribution.\n        \"\"\"\n        self.p = p\n\n    async def sample(self):\n        n = np.random.geometric(self.p)\n        return n, await self.log_prob(n)\n\n    async def log_prob(self, value):\n        return np.log(self.p) + np.log(1 - self.p) * (value - 1)\n\n    async def argmax(self, idx):\n        return idx - 1  # Most likely outcome is 0, then 1, etc.\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Geometric.__init__","title":"<code>__init__(p)</code>","text":"<p>Create a Geometric distribution.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <p>the rate of the Geometric distribution.</p> required Source code in <code>hfppl/distributions/geometric.py</code> <pre><code>def __init__(self, p):\n    \"\"\"Create a Geometric distribution.\n\n    Args:\n        p: the rate of the Geometric distribution.\n    \"\"\"\n    self.p = p\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.LMContext","title":"<code>LMContext</code>","text":"<p>Represents a generation-in-progress from a language model.</p> <p>The state tracks two pieces of information:</p> <ul> <li>A sequence of tokens \u2014 the ever-growing context for the language model.</li> <li>A current mask \u2014 a set of tokens that have not yet been ruled out as the next token.</li> </ul> <p>Storing a mask enables sub-token generation: models can use <code>LMContext</code> to sample the next token in stages, first deciding, e.g., whether to use an upper-case or lower-case first letter, and only later deciding which upper-case or lower-case token to generate.</p> <p>The state of a <code>LMContext</code> can be advanced in two ways:</p> <ol> <li>Sampling, observing, or intervening the <code>next_token()</code> distribution. This causes a token to be added to the growing sequence of tokens. Supports auto-batching.</li> <li>Sampling, observing, or intervening the <code>mask_dist(mask)</code> distribution for a given mask (set of token ids). This changes the current mask.</li> </ol> <p>Attributes:</p> Name Type Description <code>lm</code> <code>CachedCausalLM</code> <p>the language model for which this is a context</p> <code>tokens</code> <code>list[int]</code> <p>the underlying sequence of tokens, including prompt, in this context</p> <code>next_token_logprobs</code> <code>array</code> <p>numpy array holding the log probabilities for the next token. Unlike the log probabilities reported by <code>CachedCausalLM.next_token_logprobs</code>, these probabilities are rescaled for this <code>LMContext</code>'s temperature parameter, and for any active masks. This vector is managed by the <code>LMContext</code> object internally; do not mutate.</p> <code>temp</code> <code>float</code> <p>temeprature for next-token distribution (0 &lt; temp &lt; float('inf'))</p> <code>model_mask</code> <code>set[int]</code> <p>set of tokens that have not been ruled out as the next token. This mask is managed by the <code>LMContext</code> object internally; do not mutate.</p> <code>show_prompt</code> <code>bool</code> <p>controls whether the string representation of this <code>LMContext</code> includes the initial prompt or not. Defaults to <code>False</code>.</p> Source code in <code>hfppl/distributions/lmcontext.py</code> <pre><code>class LMContext:\n    \"\"\"Represents a generation-in-progress from a language model.\n\n    The state tracks two pieces of information:\n\n    * A sequence of tokens \u2014 the ever-growing context for the language model.\n    * A *current mask* \u2014 a set of tokens that have not yet been ruled out as the next token.\n\n    Storing a mask enables _sub-token_ generation: models can use `LMContext` to sample\n    the next token in _stages_, first deciding, e.g., whether to use an upper-case or lower-case\n    first letter, and only later deciding which upper-case or lower-case token to generate.\n\n    The state of a `LMContext` can be advanced in two ways:\n\n    1. Sampling, observing, or intervening the `next_token()` distribution. This causes a token\n    to be added to the growing sequence of tokens. Supports auto-batching.\n    2. Sampling, observing, or intervening the `mask_dist(mask)` distribution for a given mask (set of\n    token ids). This changes the current mask.\n\n    Attributes:\n        lm (hfppl.llms.CachedCausalLM): the language model for which this is a context\n        tokens (list[int]): the underlying sequence of tokens, including prompt, in this context\n        next_token_logprobs (numpy.array): numpy array holding the log probabilities for the next token. Unlike the log probabilities reported by `CachedCausalLM.next_token_logprobs`, these probabilities are rescaled for this `LMContext`'s temperature parameter, and for any active masks. This vector is managed by the `LMContext` object internally; do not mutate.\n        temp (float): temeprature for next-token distribution (0 &lt; temp &lt; float('inf'))\n        model_mask (set[int]): set of tokens that have not been ruled out as the next token. This mask is managed by the `LMContext` object internally; do not mutate.\n        show_prompt (bool): controls whether the string representation of this `LMContext` includes the initial prompt or not. Defaults to `False`.\n    \"\"\"\n\n    def __init__(self, lm, prompt, temp=1.0, show_prompt=False, show_eos=True):\n        \"\"\"Create a new `LMContext` with a given prompt and temperature.\n\n        Args:\n            lm (hfppl.llms.CachedCausalLM): the language model for which this is a context.\n            prompt (str): a string with which to initialize the context. Will be tokenized using `lm.tokenizer`.\n            temp (float): temeprature for next-token distribution (0 &lt; temp &lt; float('inf'))\n\n        Note:\n            For async initialization of LMContext, use LMContext.create().\n        \"\"\"\n        self._init_common(lm, prompt, temp, show_prompt, show_eos)\n        self.next_token_logprobs = log_softmax(\n            lm.next_token_logprobs_unbatched(self.tokens) / temp\n        )\n\n    @classmethod\n    async def create(cls, lm, prompt, temp=1.0, show_prompt=False, show_eos=True):\n        \"\"\"Asynchronously create a new `LMContext` with a given prompt and temperature.\"\"\"\n        self = cls.__new__(cls)\n        self._init_common(lm, prompt, temp, show_prompt, show_eos)\n        logprobs = await lm.next_token_logprobs(self.tokens)\n        self.next_token_logprobs = log_softmax(logprobs / temp)\n        return self\n\n    def _init_common(self, lm, prompt, temp, show_prompt, show_eos):\n        \"\"\"Initialize common attributes shared between __init__ and create.\"\"\"\n        self.lm = lm\n        self.tokens = lm.tokenizer.encode(prompt)\n        self.temp = temp\n        self.model_mask = lm.masks.ALL_TOKENS\n        self.prompt_string_length = len(lm.tokenizer.decode(self.tokens))\n        self.prompt_token_count = len(self.tokens)\n        self.show_prompt = show_prompt\n        self.show_eos = show_eos\n\n    def next_token(self):\n        \"\"\"Distribution over the next token.\n\n        Sampling or observing from this distribution advances the state of this `LMContext` instance.\n        \"\"\"\n        return LMNextToken(self)\n\n    def mask_dist(self, mask):\n        \"\"\"Bernoulli distribution, with probability of True equal to the probability that the next token of this `LMContext` belongs\n        to the given mask.\n\n        Sampling or observing from this distribution modifies the state of this `LMContext` instance, so that\n        the `next_token()` distribution either *will* (if True) or *will not* (if False) generate a token from\n        the given mask.\n\n        Args:\n            mask: a `set(int)` specifying which token ids are included within the mask.\n        \"\"\"\n        return LMTokenMask(self, mask)\n\n    @property\n    def token_count(self):\n        return len(self.tokens) - self.prompt_token_count\n\n    def __str__(self):\n        full_string = self.lm.tokenizer.decode(self.tokens)\n        if not self.show_prompt:\n            full_string = full_string[self.prompt_string_length :]\n        if not self.show_eos and full_string.endswith(self.lm.tokenizer.eos_token):\n            full_string = full_string[: -len(self.lm.tokenizer.eos_token)]\n        return full_string\n\n    def __deepcopy__(self, memo):\n        cpy = type(self).__new__(type(self))\n\n        for k, v in self.__dict__.items():\n            if k in set([\"lm\"]):\n                setattr(cpy, k, v)\n            else:\n                setattr(cpy, k, copy.deepcopy(v, memo))\n\n        return cpy\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.LMContext.__init__","title":"<code>__init__(lm, prompt, temp=1.0, show_prompt=False, show_eos=True)</code>","text":"<p>Create a new <code>LMContext</code> with a given prompt and temperature.</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <code>CachedCausalLM</code> <p>the language model for which this is a context.</p> required <code>prompt</code> <code>str</code> <p>a string with which to initialize the context. Will be tokenized using <code>lm.tokenizer</code>.</p> required <code>temp</code> <code>float</code> <p>temeprature for next-token distribution (0 &lt; temp &lt; float('inf'))</p> <code>1.0</code> Note <p>For async initialization of LMContext, use LMContext.create().</p> Source code in <code>hfppl/distributions/lmcontext.py</code> <pre><code>def __init__(self, lm, prompt, temp=1.0, show_prompt=False, show_eos=True):\n    \"\"\"Create a new `LMContext` with a given prompt and temperature.\n\n    Args:\n        lm (hfppl.llms.CachedCausalLM): the language model for which this is a context.\n        prompt (str): a string with which to initialize the context. Will be tokenized using `lm.tokenizer`.\n        temp (float): temeprature for next-token distribution (0 &lt; temp &lt; float('inf'))\n\n    Note:\n        For async initialization of LMContext, use LMContext.create().\n    \"\"\"\n    self._init_common(lm, prompt, temp, show_prompt, show_eos)\n    self.next_token_logprobs = log_softmax(\n        lm.next_token_logprobs_unbatched(self.tokens) / temp\n    )\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.LMContext.create","title":"<code>create(lm, prompt, temp=1.0, show_prompt=False, show_eos=True)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Asynchronously create a new <code>LMContext</code> with a given prompt and temperature.</p> Source code in <code>hfppl/distributions/lmcontext.py</code> <pre><code>@classmethod\nasync def create(cls, lm, prompt, temp=1.0, show_prompt=False, show_eos=True):\n    \"\"\"Asynchronously create a new `LMContext` with a given prompt and temperature.\"\"\"\n    self = cls.__new__(cls)\n    self._init_common(lm, prompt, temp, show_prompt, show_eos)\n    logprobs = await lm.next_token_logprobs(self.tokens)\n    self.next_token_logprobs = log_softmax(logprobs / temp)\n    return self\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.LMContext.mask_dist","title":"<code>mask_dist(mask)</code>","text":"<p>Bernoulli distribution, with probability of True equal to the probability that the next token of this <code>LMContext</code> belongs to the given mask.</p> <p>Sampling or observing from this distribution modifies the state of this <code>LMContext</code> instance, so that the <code>next_token()</code> distribution either will (if True) or will not (if False) generate a token from the given mask.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <p>a <code>set(int)</code> specifying which token ids are included within the mask.</p> required Source code in <code>hfppl/distributions/lmcontext.py</code> <pre><code>def mask_dist(self, mask):\n    \"\"\"Bernoulli distribution, with probability of True equal to the probability that the next token of this `LMContext` belongs\n    to the given mask.\n\n    Sampling or observing from this distribution modifies the state of this `LMContext` instance, so that\n    the `next_token()` distribution either *will* (if True) or *will not* (if False) generate a token from\n    the given mask.\n\n    Args:\n        mask: a `set(int)` specifying which token ids are included within the mask.\n    \"\"\"\n    return LMTokenMask(self, mask)\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.LMContext.next_token","title":"<code>next_token()</code>","text":"<p>Distribution over the next token.</p> <p>Sampling or observing from this distribution advances the state of this <code>LMContext</code> instance.</p> Source code in <code>hfppl/distributions/lmcontext.py</code> <pre><code>def next_token(self):\n    \"\"\"Distribution over the next token.\n\n    Sampling or observing from this distribution advances the state of this `LMContext` instance.\n    \"\"\"\n    return LMNextToken(self)\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.LogCategorical","title":"<code>LogCategorical</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A Geometric distribution.</p> Source code in <code>hfppl/distributions/logcategorical.py</code> <pre><code>class LogCategorical(Distribution):\n    \"\"\"A Geometric distribution.\"\"\"\n\n    def __init__(self, logits):\n        \"\"\"Create a Categorical distribution from unnormalized log probabilities (logits).\n        Given an array of logits, takes their `softmax` and samples an integer in `range(len(logits))`\n        from the resulting categorical.\n\n        Args:\n            logits (np.array): a numpy array of unnormalized log probabilities.\n        \"\"\"\n        self.log_probs = log_softmax(logits)\n\n    async def sample(self):\n        n = np.random.choice(len(self.log_probs), p=np.exp(self.log_probs))\n        return n, await self.log_prob(n)\n\n    async def log_prob(self, value):\n        return self.log_probs[value]\n\n    async def argmax(self, idx):\n        return np.argsort(self.log_probs)[-idx]\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.LogCategorical.__init__","title":"<code>__init__(logits)</code>","text":"<p>Create a Categorical distribution from unnormalized log probabilities (logits). Given an array of logits, takes their <code>softmax</code> and samples an integer in <code>range(len(logits))</code> from the resulting categorical.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>array</code> <p>a numpy array of unnormalized log probabilities.</p> required Source code in <code>hfppl/distributions/logcategorical.py</code> <pre><code>def __init__(self, logits):\n    \"\"\"Create a Categorical distribution from unnormalized log probabilities (logits).\n    Given an array of logits, takes their `softmax` and samples an integer in `range(len(logits))`\n    from the resulting categorical.\n\n    Args:\n        logits (np.array): a numpy array of unnormalized log probabilities.\n    \"\"\"\n    self.log_probs = log_softmax(logits)\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Masks","title":"<code>Masks</code>","text":"Source code in <code>hfppl/llms.py</code> <pre><code>class Masks:\n    def __init__(self, lm):\n        self.ALL_TOKENS = set(range(len(lm.str_vocab)))\n        self.STARTS_NEW_WORD = set(\n            i\n            for (i, v) in enumerate(lm.str_vocab)\n            if v[0] == \" \"\n            and len(v) &gt; 1\n            and v[1] not in string.whitespace\n            and v[1] not in string.punctuation\n        )\n        self.CONTINUES_CURRENT_WORD = set(\n            i\n            for (i, v) in enumerate(lm.str_vocab)\n            if all(c in \"'\" or c.isalpha() for c in v)\n        )\n        self.MID_PUNCTUATION = set(\n            i for (i, v) in enumerate(lm.str_vocab) if v in (\",\", \":\", \";\", \"-\", '\"')\n        )\n        self.END_PUNCTUATION = set(\n            i for (i, v) in enumerate(lm.str_vocab) if v in (\".\", \"!\", \"?\")\n        )\n        self.PUNCTUATION = self.MID_PUNCTUATION | self.END_PUNCTUATION\n        self.CONTAINS_WHITESPACE = set(\n            i\n            for (i, v) in enumerate(lm.str_vocab)\n            if any(c in string.whitespace for c in v)\n        )\n        self.EOS = set([lm.tokenizer.eos_token_id])\n\n        self.MAX_TOKEN_LENGTH = self.precompute_token_length_masks(lm)\n\n    def precompute_token_length_masks(self, lm):\n        \"\"\"Precompute masks for tokens of different lengths.\n\n        Each mask is a set of token ids that are of the given length or shorter.\"\"\"\n        max_token_length = max([len(t) for t in lm.str_vocab])\n\n        masks = defaultdict(lambda: self.ALL_TOKENS)\n        masks[0] = set([lm.tokenizer.eos_token_id])\n        for token_length in range(1, max_token_length + 1):\n            masks[token_length] = set(\n                i\n                for (i, v) in enumerate(lm.str_vocab)\n                if len(v) &lt;= token_length and i != lm.tokenizer.eos_token_id\n            )\n\n        return masks\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Masks.precompute_token_length_masks","title":"<code>precompute_token_length_masks(lm)</code>","text":"<p>Precompute masks for tokens of different lengths.</p> <p>Each mask is a set of token ids that are of the given length or shorter.</p> Source code in <code>hfppl/llms.py</code> <pre><code>def precompute_token_length_masks(self, lm):\n    \"\"\"Precompute masks for tokens of different lengths.\n\n    Each mask is a set of token ids that are of the given length or shorter.\"\"\"\n    max_token_length = max([len(t) for t in lm.str_vocab])\n\n    masks = defaultdict(lambda: self.ALL_TOKENS)\n    masks[0] = set([lm.tokenizer.eos_token_id])\n    for token_length in range(1, max_token_length + 1):\n        masks[token_length] = set(\n            i\n            for (i, v) in enumerate(lm.str_vocab)\n            if len(v) &lt;= token_length and i != lm.tokenizer.eos_token_id\n        )\n\n    return masks\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Model","title":"<code>Model</code>","text":"<p>Base class for all LLaMPPL models.</p> <p>Your models should subclass this class. Minimally, you should provide an <code>__init__</code> method that calls <code>super().__init__(self)</code>, and a <code>step</code> method.</p> Source code in <code>hfppl/modeling.py</code> <pre><code>class Model:\n    \"\"\"Base class for all LLaMPPL models.\n\n    Your models should subclass this class. Minimally, you should provide an `__init__` method\n    that calls `super().__init__(self)`, and a `step` method.\n    \"\"\"\n\n    def __init__(self):\n        self.weight = 0.0\n        self.finished = False\n        self.mode = \"sample\"\n        self.beam_idx = 0\n        self.force_eos = False\n        self.twist_amount = 0.0\n\n    def reset(self):\n        self.weight = 0.0\n        self.finished = False\n        self.mode = \"sample\"\n        self.beam_idx = 0\n        self.force_eos = False\n        self.twist_amount = 0.0\n\n    def immutable_properties(self):\n        \"\"\"Return a `set[str]` of properties that LLaMPPL may assume do not change during execution of `step`.\n        This set is empty by default but can be overridden by subclasses to speed up inference.\n\n        Returns:\n            properties (set[str]): a set of immutable property names\"\"\"\n        return set()\n\n    def __deepcopy__(self, memo):\n        cpy = type(self).__new__(type(self))\n        immutable = self.immutable_properties()\n\n        for k, v in self.__dict__.items():\n            if k in immutable:\n                setattr(cpy, k, v)\n            else:\n                setattr(cpy, k, copy.deepcopy(v, memo))\n\n        return cpy\n\n    def twist(self, amt):\n        \"\"\"Multiply this particle's weight by `exp(amt)`, but divide it back out before the next `step`.\n\n        Use this method to provide heuristic guidance about whether a particle is \"on the right track\"\n        without changing the ultimate target distribution.\n\n        Args:\n            amt: the logarithm of the amount by which to (temporarily) multiply this particle's weight.\n        \"\"\"\n        self.twist_amount += amt\n        self.score(amt)\n\n    def untwist(self):\n        self.score(-self.twist_amount)\n        self.twist_amount = 0.0\n\n    def finish(self):\n        self.untwist()\n        self.finished = True\n\n    def done_stepping(self):\n        return self.finished\n\n    async def step(self):\n        \"\"\"Defines the computation performed in each step of the model.\n\n        All subclasses should override this method.\"\"\"\n\n        if not self.done_stepping():\n            raise NotImplementedError(\"Model.step() must be implemented by subclasses\")\n\n    def __str__(self):\n        return \"Particle\"\n\n    async def start(self):\n        pass\n\n    def score(self, score):\n        \"\"\"Multiply this particle's weight by `exp(score)`.\n\n        The `score` method is a low-level way to change the target distribution.\n        For many use cases, it is sufficient to use `sample`, `observe`, `condition`,\n        and `twist`, all of which are implemented in terms of `score`.\n\n        Args:\n            score: logarithm of the amount by which the particle's weight should be multiplied.\n        \"\"\"\n        self.weight += score\n\n    def condition(self, b):\n        \"\"\"Constrain a given Boolean expression to be `True`.\n\n        If the condition is False, the particle's weight is set to zero and `self.finish()`\n        is called, so that no further `step` calls are made.\n\n        Args:\n            b: the Boolean expression whose value is constrained to be True.\n        \"\"\"\n        if not b:\n            self.score(float(\"-inf\"))\n            self.finish()\n\n    async def intervene(self, dist, x):\n        \"\"\"Force the distribution to take on the value `x`, but do not _condition_ on this result.\n\n        This is useful primarily with distributions that have side effects (e.g., modifying some state).\n        For example, a model with the code\n\n        ```python\n        token_1 = await self.sample(self.stateful_lm.next_token())\n        await self.observe(self.stateful_lm.next_token(), token_2)\n        ```\n\n        encodes a posterior inference problem, to find `token_1` values that *likely preceded* `token_2`. By contrast,\n\n        ```python\n        token_1 = await self.sample(stateful_lm.next_token())\n        await self.intervene(self.stateful_lm.next_token(), token_2)\n        ```\n\n        encodes a much easier task: freely generate `token_1` and then force-feed `token_2` as the following token.\n\n        Args:\n            dist (hfppl.distributions.distribution.Distribution): the distribution on which to intervene.\n            x: the value to intervene with.\n        \"\"\"\n        await dist.log_prob(x)\n        return x\n\n    async def observe(self, dist, x):\n        \"\"\"Condition the model on the value `x` being sampled from the distribution `dist`.\n\n        For discrete distributions `dist`, `await self.observe(dist, x)` specifies the same constraint as\n        ```\n        val = await self.sample(dist)\n        self.condition(val == x)\n        ```\n        but can be much more efficient.\n\n        Args:\n            dist: a `Distribution` object from which to observe\n            x: the value observed from `dist`\n        \"\"\"\n        p = await dist.log_prob(x)\n        self.score(p)\n        return x\n\n    async def sample(self, dist, proposal=None):\n        \"\"\"Extend the model with a sample from a given `Distribution`, with support for autobatching.\n        If specified, the Distribution `proposal` is used during inference to generate informed hypotheses.\n\n        Args:\n            dist: the `Distribution` object from which to sample\n            proposal: if provided, inference algorithms will use this `Distribution` object to generate proposed samples, rather than `dist`.\n              However, importance weights will be adjusted so that the target posterior is independent of the proposal.\n\n        Returns:\n            value: the value sampled from the distribution.\n        \"\"\"\n        # Special logic for beam search\n        # if self.mode == \"beam\":\n        #     d = dist if proposal is None else proposal\n        #     x, w = d.argmax(self.beam_idx)\n        #     if proposal is not None:\n        #         self.score(dist.log_prob(x))\n        #     else:\n        #         self.score(w)\n        #     return x\n\n        if proposal is None:\n            x, _ = await dist.sample()\n            return x\n        else:\n            x, q = await proposal.sample()\n            p = await dist.log_prob(x)\n            self.score(p - q)\n            return x\n\n    async def call(self, submodel):\n        return await submodel.run_with_parent(self)\n\n    def string_for_serialization(self):\n        \"\"\"Return a string representation of the particle for serialization purposes.\n\n        Returns:\n            str: a string representation of the particle.\n        \"\"\"\n        return str(self)\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Model.condition","title":"<code>condition(b)</code>","text":"<p>Constrain a given Boolean expression to be <code>True</code>.</p> <p>If the condition is False, the particle's weight is set to zero and <code>self.finish()</code> is called, so that no further <code>step</code> calls are made.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <p>the Boolean expression whose value is constrained to be True.</p> required Source code in <code>hfppl/modeling.py</code> <pre><code>def condition(self, b):\n    \"\"\"Constrain a given Boolean expression to be `True`.\n\n    If the condition is False, the particle's weight is set to zero and `self.finish()`\n    is called, so that no further `step` calls are made.\n\n    Args:\n        b: the Boolean expression whose value is constrained to be True.\n    \"\"\"\n    if not b:\n        self.score(float(\"-inf\"))\n        self.finish()\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Model.immutable_properties","title":"<code>immutable_properties()</code>","text":"<p>Return a <code>set[str]</code> of properties that LLaMPPL may assume do not change during execution of <code>step</code>. This set is empty by default but can be overridden by subclasses to speed up inference.</p> <p>Returns:</p> Name Type Description <code>properties</code> <code>set[str]</code> <p>a set of immutable property names</p> Source code in <code>hfppl/modeling.py</code> <pre><code>def immutable_properties(self):\n    \"\"\"Return a `set[str]` of properties that LLaMPPL may assume do not change during execution of `step`.\n    This set is empty by default but can be overridden by subclasses to speed up inference.\n\n    Returns:\n        properties (set[str]): a set of immutable property names\"\"\"\n    return set()\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Model.intervene","title":"<code>intervene(dist, x)</code>  <code>async</code>","text":"<p>Force the distribution to take on the value <code>x</code>, but do not condition on this result.</p> <p>This is useful primarily with distributions that have side effects (e.g., modifying some state). For example, a model with the code</p> <pre><code>token_1 = await self.sample(self.stateful_lm.next_token())\nawait self.observe(self.stateful_lm.next_token(), token_2)\n</code></pre> <p>encodes a posterior inference problem, to find <code>token_1</code> values that likely preceded <code>token_2</code>. By contrast,</p> <pre><code>token_1 = await self.sample(stateful_lm.next_token())\nawait self.intervene(self.stateful_lm.next_token(), token_2)\n</code></pre> <p>encodes a much easier task: freely generate <code>token_1</code> and then force-feed <code>token_2</code> as the following token.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>Distribution</code> <p>the distribution on which to intervene.</p> required <code>x</code> <p>the value to intervene with.</p> required Source code in <code>hfppl/modeling.py</code> <pre><code>async def intervene(self, dist, x):\n    \"\"\"Force the distribution to take on the value `x`, but do not _condition_ on this result.\n\n    This is useful primarily with distributions that have side effects (e.g., modifying some state).\n    For example, a model with the code\n\n    ```python\n    token_1 = await self.sample(self.stateful_lm.next_token())\n    await self.observe(self.stateful_lm.next_token(), token_2)\n    ```\n\n    encodes a posterior inference problem, to find `token_1` values that *likely preceded* `token_2`. By contrast,\n\n    ```python\n    token_1 = await self.sample(stateful_lm.next_token())\n    await self.intervene(self.stateful_lm.next_token(), token_2)\n    ```\n\n    encodes a much easier task: freely generate `token_1` and then force-feed `token_2` as the following token.\n\n    Args:\n        dist (hfppl.distributions.distribution.Distribution): the distribution on which to intervene.\n        x: the value to intervene with.\n    \"\"\"\n    await dist.log_prob(x)\n    return x\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Model.observe","title":"<code>observe(dist, x)</code>  <code>async</code>","text":"<p>Condition the model on the value <code>x</code> being sampled from the distribution <code>dist</code>.</p> <p>For discrete distributions <code>dist</code>, <code>await self.observe(dist, x)</code> specifies the same constraint as <pre><code>val = await self.sample(dist)\nself.condition(val == x)\n</code></pre> but can be much more efficient.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <p>a <code>Distribution</code> object from which to observe</p> required <code>x</code> <p>the value observed from <code>dist</code></p> required Source code in <code>hfppl/modeling.py</code> <pre><code>async def observe(self, dist, x):\n    \"\"\"Condition the model on the value `x` being sampled from the distribution `dist`.\n\n    For discrete distributions `dist`, `await self.observe(dist, x)` specifies the same constraint as\n    ```\n    val = await self.sample(dist)\n    self.condition(val == x)\n    ```\n    but can be much more efficient.\n\n    Args:\n        dist: a `Distribution` object from which to observe\n        x: the value observed from `dist`\n    \"\"\"\n    p = await dist.log_prob(x)\n    self.score(p)\n    return x\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Model.sample","title":"<code>sample(dist, proposal=None)</code>  <code>async</code>","text":"<p>Extend the model with a sample from a given <code>Distribution</code>, with support for autobatching. If specified, the Distribution <code>proposal</code> is used during inference to generate informed hypotheses.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <p>the <code>Distribution</code> object from which to sample</p> required <code>proposal</code> <p>if provided, inference algorithms will use this <code>Distribution</code> object to generate proposed samples, rather than <code>dist</code>. However, importance weights will be adjusted so that the target posterior is independent of the proposal.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>value</code> <p>the value sampled from the distribution.</p> Source code in <code>hfppl/modeling.py</code> <pre><code>async def sample(self, dist, proposal=None):\n    \"\"\"Extend the model with a sample from a given `Distribution`, with support for autobatching.\n    If specified, the Distribution `proposal` is used during inference to generate informed hypotheses.\n\n    Args:\n        dist: the `Distribution` object from which to sample\n        proposal: if provided, inference algorithms will use this `Distribution` object to generate proposed samples, rather than `dist`.\n          However, importance weights will be adjusted so that the target posterior is independent of the proposal.\n\n    Returns:\n        value: the value sampled from the distribution.\n    \"\"\"\n    # Special logic for beam search\n    # if self.mode == \"beam\":\n    #     d = dist if proposal is None else proposal\n    #     x, w = d.argmax(self.beam_idx)\n    #     if proposal is not None:\n    #         self.score(dist.log_prob(x))\n    #     else:\n    #         self.score(w)\n    #     return x\n\n    if proposal is None:\n        x, _ = await dist.sample()\n        return x\n    else:\n        x, q = await proposal.sample()\n        p = await dist.log_prob(x)\n        self.score(p - q)\n        return x\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Model.score","title":"<code>score(score)</code>","text":"<p>Multiply this particle's weight by <code>exp(score)</code>.</p> <p>The <code>score</code> method is a low-level way to change the target distribution. For many use cases, it is sufficient to use <code>sample</code>, <code>observe</code>, <code>condition</code>, and <code>twist</code>, all of which are implemented in terms of <code>score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <p>logarithm of the amount by which the particle's weight should be multiplied.</p> required Source code in <code>hfppl/modeling.py</code> <pre><code>def score(self, score):\n    \"\"\"Multiply this particle's weight by `exp(score)`.\n\n    The `score` method is a low-level way to change the target distribution.\n    For many use cases, it is sufficient to use `sample`, `observe`, `condition`,\n    and `twist`, all of which are implemented in terms of `score`.\n\n    Args:\n        score: logarithm of the amount by which the particle's weight should be multiplied.\n    \"\"\"\n    self.weight += score\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Model.step","title":"<code>step()</code>  <code>async</code>","text":"<p>Defines the computation performed in each step of the model.</p> <p>All subclasses should override this method.</p> Source code in <code>hfppl/modeling.py</code> <pre><code>async def step(self):\n    \"\"\"Defines the computation performed in each step of the model.\n\n    All subclasses should override this method.\"\"\"\n\n    if not self.done_stepping():\n        raise NotImplementedError(\"Model.step() must be implemented by subclasses\")\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Model.string_for_serialization","title":"<code>string_for_serialization()</code>","text":"<p>Return a string representation of the particle for serialization purposes.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>a string representation of the particle.</p> Source code in <code>hfppl/modeling.py</code> <pre><code>def string_for_serialization(self):\n    \"\"\"Return a string representation of the particle for serialization purposes.\n\n    Returns:\n        str: a string representation of the particle.\n    \"\"\"\n    return str(self)\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Model.twist","title":"<code>twist(amt)</code>","text":"<p>Multiply this particle's weight by <code>exp(amt)</code>, but divide it back out before the next <code>step</code>.</p> <p>Use this method to provide heuristic guidance about whether a particle is \"on the right track\" without changing the ultimate target distribution.</p> <p>Parameters:</p> Name Type Description Default <code>amt</code> <p>the logarithm of the amount by which to (temporarily) multiply this particle's weight.</p> required Source code in <code>hfppl/modeling.py</code> <pre><code>def twist(self, amt):\n    \"\"\"Multiply this particle's weight by `exp(amt)`, but divide it back out before the next `step`.\n\n    Use this method to provide heuristic guidance about whether a particle is \"on the right track\"\n    without changing the ultimate target distribution.\n\n    Args:\n        amt: the logarithm of the amount by which to (temporarily) multiply this particle's weight.\n    \"\"\"\n    self.twist_amount += amt\n    self.score(amt)\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Token","title":"<code>Token</code>","text":"<p>Class representing a token.</p> <p>Attributes:</p> Name Type Description <code>lm</code> <code>CachedCausalLM</code> <p>the language model for which this is a Token.</p> <code>token_id</code> <code>int</code> <p>the integer token id (an index into the vocabulary).</p> <code>token_str</code> <code>str</code> <p>a string, which the token represents\u2014equal to <code>lm.str_vocab[token_id]</code>.</p> Source code in <code>hfppl/llms.py</code> <pre><code>class Token:\n    \"\"\"Class representing a token.\n\n    Attributes:\n        lm (hfppl.llms.CachedCausalLM): the language model for which this is a Token.\n        token_id (int): the integer token id (an index into the vocabulary).\n        token_str (str): a string, which the token represents\u2014equal to `lm.str_vocab[token_id]`.\n    \"\"\"\n\n    def __init__(self, lm, token_id, token_str):\n        self.lm = lm\n        self.token_id = token_id\n        self.token_str = token_str\n\n    # Adding tokens\n    def __add__(self, other):\n        s = TokenSequence(self.lm, [self.token_id])\n        s += other\n        return s\n\n    def __radd__(self, other):\n        s = TokenSequence(self.lm, [self.token_id])\n        return other + s\n\n    # Support checking for EOS\n    def __eq__(self, other):\n        if isinstance(other, Token):\n            return self.lm is other.lm and self.token_id == other.token_id\n        elif isinstance(other, int):\n            return self.token_id == other\n        else:\n            return self.token_str == other\n\n    def __int__(self):\n        return self.token_id\n\n    def __str__(self):\n        return self.token_str\n\n    def __repr__(self):\n        return f\"&lt;{self.token_str}|{self.token_id}&gt;\"\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.TokenCategorical","title":"<code>TokenCategorical</code>","text":"<p>               Bases: <code>Distribution</code></p> Source code in <code>hfppl/distributions/tokencategorical.py</code> <pre><code>class TokenCategorical(Distribution):\n    def __init__(self, lm, logits):\n        \"\"\"Create a Categorical distribution whose values are Tokens, not integers.\n        Given a language model `lm` and an array of unnormalized log probabilities (of length `len(lm.vocab)`),\n        uses softmax to normalize them and samples a Token from the resulting categorical.\n\n        Args:\n            lm (hfppl.llms.CachedCausalLM): the language model whose vocabulary is to be generated from.\n            logits (np.array): a numpy array of unnormalized log probabilities.\n        \"\"\"\n        self.lm = lm\n        self.log_probs = log_softmax(logits)\n        if self.lm.tokenizer.vocab_size != len(logits):\n            raise RuntimeError(\n                f\"TokenCategorical: vocab size is {self.lm.tokenizer.vocab_size} but provided {len(logits)} logits.\"\n            )\n\n    async def sample(self):\n        n = np.random.choice(len(self.log_probs), p=(np.exp(self.log_probs)))\n        return (\n            Token(self.lm, n, self.lm.tokenizer.convert_ids_to_tokens(n)),\n            self.log_probs[n],\n        )\n\n    async def log_prob(self, value):\n        return self.log_probs[value.token_id]\n\n    async def argmax(self, idx):\n        tok = torch.argsort(self.log_probs)[-idx]\n        return (\n            Token(self.lm, tok, self.lm.tokenizer.convert_ids_to_tokens(tok)),\n            self.log_probs[tok],\n        )\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.TokenCategorical.__init__","title":"<code>__init__(lm, logits)</code>","text":"<p>Create a Categorical distribution whose values are Tokens, not integers. Given a language model <code>lm</code> and an array of unnormalized log probabilities (of length <code>len(lm.vocab)</code>), uses softmax to normalize them and samples a Token from the resulting categorical.</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <code>CachedCausalLM</code> <p>the language model whose vocabulary is to be generated from.</p> required <code>logits</code> <code>array</code> <p>a numpy array of unnormalized log probabilities.</p> required Source code in <code>hfppl/distributions/tokencategorical.py</code> <pre><code>def __init__(self, lm, logits):\n    \"\"\"Create a Categorical distribution whose values are Tokens, not integers.\n    Given a language model `lm` and an array of unnormalized log probabilities (of length `len(lm.vocab)`),\n    uses softmax to normalize them and samples a Token from the resulting categorical.\n\n    Args:\n        lm (hfppl.llms.CachedCausalLM): the language model whose vocabulary is to be generated from.\n        logits (np.array): a numpy array of unnormalized log probabilities.\n    \"\"\"\n    self.lm = lm\n    self.log_probs = log_softmax(logits)\n    if self.lm.tokenizer.vocab_size != len(logits):\n        raise RuntimeError(\n            f\"TokenCategorical: vocab size is {self.lm.tokenizer.vocab_size} but provided {len(logits)} logits.\"\n        )\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.TokenSequence","title":"<code>TokenSequence</code>","text":"<p>A sequence of tokens.</p> <p>Supports addition (via <code>+</code> or mutating <code>+=</code>) with:</p> <ul> <li>other <code>TokenSequence</code> instances (concatenation)</li> <li>individual tokens, represented as integers or <code>Token</code> instances</li> <li>strings, which are tokenized by <code>lm.tokenizer</code></li> </ul> <p>Attributes:</p> Name Type Description <code>lm</code> <code>CachedCausalLM</code> <p>the language model whose vocabulary the tokens come from.</p> <code>seq</code> <code>list[Token]</code> <p>the sequence of tokens.</p> Source code in <code>hfppl/llms.py</code> <pre><code>class TokenSequence:\n    \"\"\"A sequence of tokens.\n\n    Supports addition (via `+` or mutating `+=`) with:\n\n    * other `TokenSequence` instances (concatenation)\n    * individual tokens, represented as integers or `Token` instances\n    * strings, which are tokenized by `lm.tokenizer`\n\n    Attributes:\n        lm (hfppl.llms.CachedCausalLM): the language model whose vocabulary the tokens come from.\n        seq (list[hfppl.llms.Token]): the sequence of tokens.\"\"\"\n\n    def __init__(self, lm, seq=None):\n        \"\"\"Create a `TokenSequence` from a language model and a sequence.\n\n        Args:\n            lm (hfppl.llms.CachedCausalLM): the language model whose vocabulary the tokens come from.\n            seq (str | list[int]): the sequence of token ids, or a string which will be automatically tokenized. Defaults to the singleton sequence containing a bos token.\n        \"\"\"\n        self.lm = lm\n        if seq is None:\n            self.seq = [lm.tokenizer.bos_token_id]\n        elif isinstance(seq, str):\n            self.seq = self.lm.tokenizer.encode(seq)\n        else:\n            self.seq = seq\n\n    def __str__(self):\n        return self.lm.tokenizer.decode(self.seq)\n\n    def __iadd__(self, other):\n        if isinstance(other, Token):\n            assert other.lm is self.lm\n            self.seq.append(other.token_id)\n        elif isinstance(other, TokenSequence):\n            assert other.lm is self.lm\n            self.seq.extend(other.seq)\n        elif isinstance(other, str):\n            self.seq.extend(self.lm.tokenizer.encode(other, add_special_tokens=False))\n        elif isinstance(other, int):\n            self.seq.append(other)\n        else:\n            raise RuntimeError(f\"Addition not supported on {type(other)}\")\n        return self\n\n    def __radd__(self, other):\n        if isinstance(other, Token):\n            assert other.lm is self.lm\n            return TokenSequence(self.lm, [other.token_id, *self.seq])\n        elif isinstance(other, TokenSequence):\n            assert other.lm is self.lm\n            return TokenSequence(self.lm, other.seq + self.seq)\n        elif isinstance(other, str):\n            return TokenSequence(\n                self.lm,\n                self.lm.tokenizer.encode(other, add_special_tokens=False) + self.seq,\n            )\n        elif isinstance(other, int):\n            return TokenSequence(self.lm, [other, *self.seq])\n        else:\n            raise RuntimeError(f\"Addition not supported on {type(other)}\")\n\n    def __add__(self, other):\n        s = TokenSequence(self.lm, self.seq)\n        s += other\n        return s\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.TokenSequence.__init__","title":"<code>__init__(lm, seq=None)</code>","text":"<p>Create a <code>TokenSequence</code> from a language model and a sequence.</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <code>CachedCausalLM</code> <p>the language model whose vocabulary the tokens come from.</p> required <code>seq</code> <code>str | list[int]</code> <p>the sequence of token ids, or a string which will be automatically tokenized. Defaults to the singleton sequence containing a bos token.</p> <code>None</code> Source code in <code>hfppl/llms.py</code> <pre><code>def __init__(self, lm, seq=None):\n    \"\"\"Create a `TokenSequence` from a language model and a sequence.\n\n    Args:\n        lm (hfppl.llms.CachedCausalLM): the language model whose vocabulary the tokens come from.\n        seq (str | list[int]): the sequence of token ids, or a string which will be automatically tokenized. Defaults to the singleton sequence containing a bos token.\n    \"\"\"\n    self.lm = lm\n    if seq is None:\n        self.seq = [lm.tokenizer.bos_token_id]\n    elif isinstance(seq, str):\n        self.seq = self.lm.tokenizer.encode(seq)\n    else:\n        self.seq = seq\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Transformer","title":"<code>Transformer</code>","text":"<p>               Bases: <code>Distribution</code></p> Source code in <code>hfppl/distributions/transformer.py</code> <pre><code>class Transformer(Distribution):\n    def __init__(self, lm, prompt, temp=1.0):\n        \"\"\"Create a Categorical distribution whose values are Tokens, with probabilities given\n        by a language model. Supports auto-batching.\n\n        Args:\n            lm (hfppl.llms.CachedCausalLM): the language model.\n            prompt (str | hfppl.llms.TokenSequence): the sequence of tokens to use as the prompt. If a string, `lm.tokenizer` is used to encode it.\n            temp (float): temperature at which to generate (0 &lt; `temp` &lt; `float('inf')`).\n        \"\"\"\n        self.lm = lm\n        self.temp = temp\n\n        # prompt will be a list of ints\n        if isinstance(prompt, str):\n            prompt = self.lm.tokenizer.encode(prompt)\n        elif isinstance(prompt, TokenSequence):\n            prompt = prompt.seq\n\n        self.prompt = prompt\n\n    async def log_prob(self, x):\n        log_probs = await self.lm.next_token_logprobs(self.prompt)\n        log_probs = log_probs / self.temp\n\n        if isinstance(x, Token):\n            x = x.token_id\n\n        return log_probs[x]\n\n    async def sample(self):\n        log_probs = await self.lm.next_token_logprobs(self.prompt)\n        log_probs = log_probs / self.temp\n        probs = np.exp(log_probs)\n        token_id = np.random.choice(len(probs), p=(probs))\n        logprob = log_probs[token_id]\n        return (\n            Token(self.lm, token_id, self.lm.tokenizer.convert_ids_to_tokens(token_id)),\n            logprob,\n        )\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.Transformer.__init__","title":"<code>__init__(lm, prompt, temp=1.0)</code>","text":"<p>Create a Categorical distribution whose values are Tokens, with probabilities given by a language model. Supports auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <code>CachedCausalLM</code> <p>the language model.</p> required <code>prompt</code> <code>str | TokenSequence</code> <p>the sequence of tokens to use as the prompt. If a string, <code>lm.tokenizer</code> is used to encode it.</p> required <code>temp</code> <code>float</code> <p>temperature at which to generate (0 &lt; <code>temp</code> &lt; <code>float('inf')</code>).</p> <code>1.0</code> Source code in <code>hfppl/distributions/transformer.py</code> <pre><code>def __init__(self, lm, prompt, temp=1.0):\n    \"\"\"Create a Categorical distribution whose values are Tokens, with probabilities given\n    by a language model. Supports auto-batching.\n\n    Args:\n        lm (hfppl.llms.CachedCausalLM): the language model.\n        prompt (str | hfppl.llms.TokenSequence): the sequence of tokens to use as the prompt. If a string, `lm.tokenizer` is used to encode it.\n        temp (float): temperature at which to generate (0 &lt; `temp` &lt; `float('inf')`).\n    \"\"\"\n    self.lm = lm\n    self.temp = temp\n\n    # prompt will be a list of ints\n    if isinstance(prompt, str):\n        prompt = self.lm.tokenizer.encode(prompt)\n    elif isinstance(prompt, TokenSequence):\n        prompt = prompt.seq\n\n    self.prompt = prompt\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.log_softmax","title":"<code>log_softmax(nums)</code>","text":"<p>Compute log(softmax(nums)).</p> <p>Parameters:</p> Name Type Description Default <code>nums</code> <p>a vector or numpy array of unnormalized log probabilities.</p> required <p>Returns:</p> Type Description <p>np.array: an array of log (normalized) probabilities.</p> Source code in <code>hfppl/util.py</code> <pre><code>def log_softmax(nums):\n    \"\"\"Compute log(softmax(nums)).\n\n    Args:\n        nums: a vector or numpy array of unnormalized log probabilities.\n\n    Returns:\n        np.array: an array of log (normalized) probabilities.\n    \"\"\"\n    return nums - logsumexp(nums)\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.sample_word","title":"<code>sample_word(self, context, max_tokens=5, allow_punctuation=True)</code>  <code>async</code>","text":"<p>Sample a word from the <code>LMContext</code> object <code>context</code>.</p> Source code in <code>hfppl/chunks.py</code> <pre><code>@submodel\nasync def sample_word(self, context, max_tokens=5, allow_punctuation=True):\n    \"\"\"Sample a word from the `LMContext` object `context`.\"\"\"\n    last_token = (\n        context.lm.str_vocab[context.tokens[-1]] if len(context.tokens) &gt; 0 else \"\"\n    )\n    last_character = last_token[-1] if len(last_token) &gt; 0 else \"\"\n    needs_space = last_character not in string.whitespace and last_character not in [\n        \"-\",\n        \"'\",\n        '\"',\n    ]\n    if needs_space:\n        starts_word_mask = context.lm.masks.STARTS_NEW_WORD\n    else:\n        starts_word_mask = context.lm.masks.CONTINUES_CURRENT_WORD\n\n    # Force model to start a new word\n    await self.observe(context.mask_dist(starts_word_mask), True)\n\n    word = \"\"\n    num_tokens = 0\n    while True:\n        token = await self.sample(context.next_token())\n        word += context.lm.str_vocab[token.token_id]\n        num_tokens += 1\n\n        if num_tokens == max_tokens:\n            await self.observe(\n                context.mask_dist(context.lm.masks.CONTINUES_CURRENT_WORD), False\n            )\n            break\n\n        if not (\n            await self.sample(\n                context.mask_dist(context.lm.masks.CONTINUES_CURRENT_WORD)\n            )\n        ):\n            break\n\n    # Sample punctuation, if desired\n    punctuation = \"\"\n    if allow_punctuation and await self.sample(\n        context.mask_dist(context.lm.masks.PUNCTUATION)\n    ):\n        punctuation_token = await self.sample(context.next_token())\n        punctuation = context.lm.str_vocab[punctuation_token.token_id]\n\n    return word, punctuation\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.sample_word_2","title":"<code>sample_word_2(self, context, max_chars=None, allow_mid_punctuation=True, allow_end_punctuation=True)</code>  <code>async</code>","text":"<p>Sample a word from the <code>LMContext</code> object <code>context</code>.</p> <p>Unlike sample_word() above, this method allows for character-level control over the length of the word. It also allows for control over the presence of punctuation in the middle and at the end of the word.</p> <p>Parameters:</p> Name Type Description Default <code>max_chars</code> <code>int</code> <p>Maximum number of characters in the word. If None, the model will sample a word of any length.</p> <code>None</code> <code>allow_mid_punctuation</code> <code>bool</code> <p>If True, the model may sample punctuation in the middle of the word.</p> <code>True</code> <code>allow_end_punctuation</code> <code>bool</code> <p>If True, the model may sample punctuation at the end of the word.</p> <code>True</code> <p>Returns:</p> Type Description <p>Tuple[str, str]: The sampled word and punctuation</p> Source code in <code>hfppl/chunks.py</code> <pre><code>@submodel\nasync def sample_word_2(\n    self,\n    context,\n    max_chars: int = None,\n    allow_mid_punctuation: bool = True,\n    allow_end_punctuation: bool = True,\n):\n    \"\"\"Sample a word from the `LMContext` object `context`.\n\n    Unlike sample_word() above, this method allows for character-level control over the length of the word.\n    It also allows for control over the presence of punctuation in the middle and at the end of the word.\n\n    Args:\n        max_chars (int): Maximum number of characters in the word. If None, the model will sample a word of any length.\n        allow_mid_punctuation (bool): If True, the model may sample punctuation in the middle of the word.\n        allow_end_punctuation (bool): If True, the model may sample punctuation at the end of the word.\n\n    Returns:\n        Tuple[str, str]: The sampled word and punctuation\n    \"\"\"\n    # NOTE: Yields control back to the event loop. Necessary to allow timeouts to work correctly when this method is called in a loop.\n    await asyncio.sleep(0)\n\n    # This approach sometimes breaks with max_chars = 1\n    if max_chars is not None:\n        assert max_chars &gt; 1\n\n    last_token = (\n        context.lm.str_vocab[context.tokens[-1]] if len(context.tokens) &gt; 0 else \"\"\n    )\n    last_character = last_token[-1] if len(last_token) &gt; 0 else \"\"\n    needs_space = last_character not in string.whitespace and last_character not in [\n        \"-\",\n        \"'\",\n        '\"',\n    ]\n    if needs_space:\n        starts_word_mask = context.lm.masks.STARTS_NEW_WORD\n    else:\n        starts_word_mask = context.lm.masks.CONTINUES_CURRENT_WORD\n\n    # Force model to start a new word\n    await self.observe(context.mask_dist(starts_word_mask), True)\n\n    word = \"\"\n    while True:\n        # Force model to sample a token with an appropriate number of characters\n        if max_chars is not None:\n            await self.observe(\n                context.mask_dist(\n                    context.lm.masks.MAX_TOKEN_LENGTH[max_chars - len(word.strip())]\n                ),\n                True,\n            )\n\n        token = await self.sample(context.next_token())\n        word += context.lm.str_vocab[token.token_id]\n\n        # If we ran out of chars, break\n        if max_chars is not None and len(word.strip()) &gt;= max_chars:\n            await self.observe(\n                context.mask_dist(context.lm.masks.CONTINUES_CURRENT_WORD), False\n            )\n            break\n\n        # If the model wants to end the word, break\n        if not (\n            await self.sample(\n                context.mask_dist(context.lm.masks.CONTINUES_CURRENT_WORD)\n            )\n        ):\n            break\n\n    # Sample punctuation, if desired\n    mid_punctuation, end_punctuation = \"\", \"\"\n\n    mask = set()\n    if allow_mid_punctuation:\n        mask = mask | context.lm.masks.MID_PUNCTUATION\n    if allow_end_punctuation:\n        mask = mask | context.lm.masks.END_PUNCTUATION\n\n    if mask and await self.sample(context.mask_dist(mask)):\n        token = await self.sample(context.next_token())\n        if token.token_id in context.lm.masks.MID_PUNCTUATION:\n            mid_punctuation = context.lm.str_vocab[token.token_id]\n        if token.token_id in context.lm.masks.END_PUNCTUATION:\n            end_punctuation = context.lm.str_vocab[token.token_id]\n\n    return word, mid_punctuation, end_punctuation\n</code></pre>"},{"location":"reference/hfppl/__init__/#hfppl.submodel","title":"<code>submodel(f)</code>","text":"<p>Decorator to create a SubModel implementation from an async function.</p> <p>For example:</p> <pre><code>@submodel\nasync def sample_two_tokens(self, context):\n    token1 = await self.sample(context.next_token())\n    token2 = await self.sample(context.next_token())\n    return token1, token2\n</code></pre> <p>This SubModel can then be used from another model or submodel, using the syntax <code>await self.call(sample_two_tokens(context))</code>.</p> Source code in <code>hfppl/modeling.py</code> <pre><code>def submodel(f):\n    \"\"\"Decorator to create a SubModel implementation from an async function.\n\n    For example:\n\n    ```python\n    @submodel\n    async def sample_two_tokens(self, context):\n        token1 = await self.sample(context.next_token())\n        token2 = await self.sample(context.next_token())\n        return token1, token2\n    ```\n\n    This SubModel can then be used from another model or submodel, using the syntax `await self.call(sample_two_tokens(context))`.\n    \"\"\"\n\n    @functools.wraps(f, updated=())  # unclear if this is the best way to do it\n    class SubModelImpl(SubModel):\n        def __init__(self, *args, **kwargs):\n            super().__init__()\n            self.args = args\n            self.kwargs = kwargs\n\n        async def forward(self):\n            return await f(self, *self.args, **self.kwargs)\n\n    return SubModelImpl\n</code></pre>"},{"location":"reference/hfppl/chunks/","title":"chunks","text":""},{"location":"reference/hfppl/chunks/#hfppl.chunks.sample_word","title":"<code>sample_word(self, context, max_tokens=5, allow_punctuation=True)</code>  <code>async</code>","text":"<p>Sample a word from the <code>LMContext</code> object <code>context</code>.</p> Source code in <code>hfppl/chunks.py</code> <pre><code>@submodel\nasync def sample_word(self, context, max_tokens=5, allow_punctuation=True):\n    \"\"\"Sample a word from the `LMContext` object `context`.\"\"\"\n    last_token = (\n        context.lm.str_vocab[context.tokens[-1]] if len(context.tokens) &gt; 0 else \"\"\n    )\n    last_character = last_token[-1] if len(last_token) &gt; 0 else \"\"\n    needs_space = last_character not in string.whitespace and last_character not in [\n        \"-\",\n        \"'\",\n        '\"',\n    ]\n    if needs_space:\n        starts_word_mask = context.lm.masks.STARTS_NEW_WORD\n    else:\n        starts_word_mask = context.lm.masks.CONTINUES_CURRENT_WORD\n\n    # Force model to start a new word\n    await self.observe(context.mask_dist(starts_word_mask), True)\n\n    word = \"\"\n    num_tokens = 0\n    while True:\n        token = await self.sample(context.next_token())\n        word += context.lm.str_vocab[token.token_id]\n        num_tokens += 1\n\n        if num_tokens == max_tokens:\n            await self.observe(\n                context.mask_dist(context.lm.masks.CONTINUES_CURRENT_WORD), False\n            )\n            break\n\n        if not (\n            await self.sample(\n                context.mask_dist(context.lm.masks.CONTINUES_CURRENT_WORD)\n            )\n        ):\n            break\n\n    # Sample punctuation, if desired\n    punctuation = \"\"\n    if allow_punctuation and await self.sample(\n        context.mask_dist(context.lm.masks.PUNCTUATION)\n    ):\n        punctuation_token = await self.sample(context.next_token())\n        punctuation = context.lm.str_vocab[punctuation_token.token_id]\n\n    return word, punctuation\n</code></pre>"},{"location":"reference/hfppl/chunks/#hfppl.chunks.sample_word_2","title":"<code>sample_word_2(self, context, max_chars=None, allow_mid_punctuation=True, allow_end_punctuation=True)</code>  <code>async</code>","text":"<p>Sample a word from the <code>LMContext</code> object <code>context</code>.</p> <p>Unlike sample_word() above, this method allows for character-level control over the length of the word. It also allows for control over the presence of punctuation in the middle and at the end of the word.</p> <p>Parameters:</p> Name Type Description Default <code>max_chars</code> <code>int</code> <p>Maximum number of characters in the word. If None, the model will sample a word of any length.</p> <code>None</code> <code>allow_mid_punctuation</code> <code>bool</code> <p>If True, the model may sample punctuation in the middle of the word.</p> <code>True</code> <code>allow_end_punctuation</code> <code>bool</code> <p>If True, the model may sample punctuation at the end of the word.</p> <code>True</code> <p>Returns:</p> Type Description <p>Tuple[str, str]: The sampled word and punctuation</p> Source code in <code>hfppl/chunks.py</code> <pre><code>@submodel\nasync def sample_word_2(\n    self,\n    context,\n    max_chars: int = None,\n    allow_mid_punctuation: bool = True,\n    allow_end_punctuation: bool = True,\n):\n    \"\"\"Sample a word from the `LMContext` object `context`.\n\n    Unlike sample_word() above, this method allows for character-level control over the length of the word.\n    It also allows for control over the presence of punctuation in the middle and at the end of the word.\n\n    Args:\n        max_chars (int): Maximum number of characters in the word. If None, the model will sample a word of any length.\n        allow_mid_punctuation (bool): If True, the model may sample punctuation in the middle of the word.\n        allow_end_punctuation (bool): If True, the model may sample punctuation at the end of the word.\n\n    Returns:\n        Tuple[str, str]: The sampled word and punctuation\n    \"\"\"\n    # NOTE: Yields control back to the event loop. Necessary to allow timeouts to work correctly when this method is called in a loop.\n    await asyncio.sleep(0)\n\n    # This approach sometimes breaks with max_chars = 1\n    if max_chars is not None:\n        assert max_chars &gt; 1\n\n    last_token = (\n        context.lm.str_vocab[context.tokens[-1]] if len(context.tokens) &gt; 0 else \"\"\n    )\n    last_character = last_token[-1] if len(last_token) &gt; 0 else \"\"\n    needs_space = last_character not in string.whitespace and last_character not in [\n        \"-\",\n        \"'\",\n        '\"',\n    ]\n    if needs_space:\n        starts_word_mask = context.lm.masks.STARTS_NEW_WORD\n    else:\n        starts_word_mask = context.lm.masks.CONTINUES_CURRENT_WORD\n\n    # Force model to start a new word\n    await self.observe(context.mask_dist(starts_word_mask), True)\n\n    word = \"\"\n    while True:\n        # Force model to sample a token with an appropriate number of characters\n        if max_chars is not None:\n            await self.observe(\n                context.mask_dist(\n                    context.lm.masks.MAX_TOKEN_LENGTH[max_chars - len(word.strip())]\n                ),\n                True,\n            )\n\n        token = await self.sample(context.next_token())\n        word += context.lm.str_vocab[token.token_id]\n\n        # If we ran out of chars, break\n        if max_chars is not None and len(word.strip()) &gt;= max_chars:\n            await self.observe(\n                context.mask_dist(context.lm.masks.CONTINUES_CURRENT_WORD), False\n            )\n            break\n\n        # If the model wants to end the word, break\n        if not (\n            await self.sample(\n                context.mask_dist(context.lm.masks.CONTINUES_CURRENT_WORD)\n            )\n        ):\n            break\n\n    # Sample punctuation, if desired\n    mid_punctuation, end_punctuation = \"\", \"\"\n\n    mask = set()\n    if allow_mid_punctuation:\n        mask = mask | context.lm.masks.MID_PUNCTUATION\n    if allow_end_punctuation:\n        mask = mask | context.lm.masks.END_PUNCTUATION\n\n    if mask and await self.sample(context.mask_dist(mask)):\n        token = await self.sample(context.next_token())\n        if token.token_id in context.lm.masks.MID_PUNCTUATION:\n            mid_punctuation = context.lm.str_vocab[token.token_id]\n        if token.token_id in context.lm.masks.END_PUNCTUATION:\n            end_punctuation = context.lm.str_vocab[token.token_id]\n\n    return word, mid_punctuation, end_punctuation\n</code></pre>"},{"location":"reference/hfppl/llms/","title":"llms","text":"<p>Utilities for working with language models.</p>"},{"location":"reference/hfppl/llms/#hfppl.llms.CachedCausalLM","title":"<code>CachedCausalLM</code>","text":"<p>Wrapper around a <code>genlm_backend.llm.AsyncLM</code>.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>AsyncLM</code> <p>The underlying language model (either <code>AsyncVirtualLM</code> or <code>AsyncTransformer</code>).</p> <code>str_vocab</code> <code>list[str]</code> <p>List mapping token IDs to their string representations.</p> <code>byte_vocab</code> <code>list[bytes]</code> <p>List mapping token IDs to their byte representations.</p> <code>masks</code> <code>Masks</code> <p>Token masks for filtering logits during generation.</p> Source code in <code>hfppl/llms.py</code> <pre><code>class CachedCausalLM:\n    \"\"\"Wrapper around a [`genlm_backend.llm.AsyncLM`](https://probcomp.github.io/genlm-backend/reference/genlm_backend/llm/__init__/).\n\n    Attributes:\n        model (genlm_backend.llm.AsyncLM): The underlying language model (either `AsyncVirtualLM` or `AsyncTransformer`).\n        str_vocab (list[str]): List mapping token IDs to their string representations.\n        byte_vocab (list[bytes]): List mapping token IDs to their byte representations.\n        masks (Masks): Token masks for filtering logits during generation.\n    \"\"\"\n\n    @classmethod\n    def from_pretrained(cls, model_id, backend=None, **kwargs):\n        \"\"\"Create a CachedCausalLM from a HuggingFace model name.\n\n        This is a convenience method that instantiates the underlying `AsyncLM` from a HuggingFace model name.\n\n        Args:\n            model_id (str): Name or path of the HuggingFace pretrained model to load.\n            backend (str, optional): `AsyncLM` backend to use:\n                - 'vllm' to instantiate an `AsyncVirtualLM`; ideal for GPU usage\n                - 'hf' for an `AsyncTransformer`; ideal for CPU usage\n                - 'mock' for a `MockAsyncLM`; ideal for testing.\n                Defaults to 'vllm' if CUDA is available, otherwise 'hf'.\n            **kwargs: Additional keyword arguments passed to the `AsyncLM` constructor.\n                See [`AsyncLM` documentation](https://probcomp.github.io/genlm-backend/reference/genlm_backend/llm/__init__/).\n\n        Returns:\n            CachedCausalLM: The hfppl-compatible interface to the `AsyncLM` model.\n        \"\"\"\n        backend = backend or (\n            \"vllm\" if (torch.cuda.is_available() and VLLM_AVAILABLE) else \"hf\"\n        )\n\n        if backend == \"vllm\":\n            if not VLLM_AVAILABLE:\n                raise ValueError(\n                    \"vLLM backend requested but vLLM is not installed. \"\n                    \"Please install vLLM with `pip install vllm`.\"\n                )\n            model_cls = AsyncVirtualLM\n        elif backend == \"hf\":\n            model_cls = AsyncTransformer\n        elif backend == \"mock\":\n            model_cls = MockAsyncLM\n        else:\n            raise ValueError(\n                f\"Unknown backend: {backend}. Must be one of ['vllm', 'hf', 'mock']\"\n            )\n\n        # Handle legacy auth_token parameter. The ability to pass in the auth_token should\n        # be removed in a future version since it is not supported by the vllm backend.\n        # Users should authenticate with the HuggingFace CLI.\n        auth_token = kwargs.pop(\"auth_token\", None)\n        if auth_token:\n            if backend == \"vllm\":\n                raise ValueError(\n                    \"Explicitly passing auth_token is not compatible with the vLLM AsyncLM backend. \"\n                    \"Authenticate using `huggingface-cli login` instead.\"\n                )\n\n            if \"hf_opts\" not in kwargs:\n                kwargs[\"hf_opts\"] = {}\n            kwargs[\"hf_opts\"][\"token\"] = auth_token\n\n            warnings.warn(\n                \"Passing auth_token directly is deprecated and will be removed in a future version. \"\n                \"Please authenticate using `huggingface-cli login` instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        load_in_8bit = kwargs.pop(\"load_in_8bit\", False)\n        if load_in_8bit:\n            if \"bitsandbytes_opts\" not in kwargs:\n                kwargs[\"bitsandbytes_opts\"] = {}\n            kwargs[\"bitsandbytes_opts\"][\"load_in_8bit\"] = True\n\n            warnings.warn(\n                \"load_in_8bit is deprecated and will be removed in a future version. \"\n                \"Please pass `bitsandbytes_opts` instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        model = model_cls.from_name(model_id, **kwargs)\n\n        return cls(model)\n\n    def __init__(self, model):\n        \"\"\"\n        Create a `CachedCausalLM` from an `AsyncLM`.\n\n        Args:\n            model (genlm_backend.llm.AsyncLM): an `AsyncLM` instance.\n        \"\"\"\n        if isinstance(model, AsyncVirtualLM):\n            self.backend = \"vllm\"\n        elif isinstance(model, AsyncTransformer):\n            self.backend = \"hf\"\n        elif isinstance(model, MockAsyncLM):\n            self.backend = \"mock\"\n        else:\n            raise ValueError(\n                f\"Unknown model type: {type(model)}. Must be one of [AsyncVirtualLM, AsyncTransformer, MockAsyncLM]\"\n            )\n\n        self.model = model\n        self.tokenizer = model.tokenizer\n        self.str_vocab = model.str_vocab\n        self.byte_vocab = model.byte_vocab\n        self.masks = Masks(self)\n\n    @property\n    def vocab(self):\n        \"\"\"Legacy accessor for string vocabulary. Prefer using `.str_vocab` directly for access to the model's string vocabulary.\"\"\"\n        warnings.warn(\n            \"Accessing .vocab directly is deprecated and will be removed in a future version. Use .str_vocab or .byte_vocab instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.model.str_vocab\n\n    def __deepcopy__(self, memo):\n        return self\n\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token. This version is asynchronous and support auto batching of concurrent requests; use with `await`.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (numpy.array): a numpy array of length `len(str_vocab)` (equivalently `len(byte_vocab)`) with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        logprobs = await self.model.next_token_logprobs(token_ids)\n        return logprobs.float().cpu().numpy()\n\n    def next_token_logprobs_unbatched(self, token_ids):\n        \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (numpy.array): a numpy array of length `len(str_vocab)` (equivalently `len(byte_vocab)`) with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        return self.model.next_token_logprobs_sync(token_ids).float().cpu().numpy()\n\n    def clear_cache(self):\n        \"\"\"Clear the cache of log probabilities and key/value pairs.\n\n        For HuggingFace backend: Clears both logprob cache and KV cache.\n\n        For vLLM backend: Only clears logprob cache (KV cache is managed internally by vLLM).\n        \"\"\"\n        self.model.clear_cache()\n\n    def clear_kv_cache(self):\n        \"\"\"Clear any key and value vectors from the cache.\"\"\"\n        if self.backend == \"hf\":\n            self.model.clear_kv_cache()\n        elif self.backend == \"vllm\":\n            warnings.warn(\n                \"clear_kv_cache() is only supported for the HuggingFace backend. The KV cache for the vLLM backend is handled internally by vLLM. No operation performed.\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        elif self.backend == \"mock\":\n            pass\n        else:\n            raise RuntimeError(\n                f\"clear_kv_cache() is not implemented for backend type {type(self.model)}\"\n            )\n\n    def reset_async_queries(self):\n        \"\"\"Clear any pending language model queries from the queue.\"\"\"\n        if self.backend == \"hf\":\n            self.model.reset_async_queries()\n        elif self.backend == \"vllm\":\n            warnings.warn(\n                \"reset_async_queries() is only supported for the HuggingFace backend. No operation performed.\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        elif self.backend == \"mock\":\n            pass\n        else:\n            raise RuntimeError(\n                f\"reset_async_queries() is not implemented for backend type {type(self.model)}\"\n            )\n\n    def cache_kv(self, prompt_tokens):\n        \"\"\"Cache the key and value vectors for a prompt.\n\n        Args:\n            prompt_tokens (list[int]): token ids for the prompt to cache.\n        \"\"\"\n        if self.backend == \"hf\":\n            self.model.cache_kv(prompt_tokens)\n        elif self.backend == \"vllm\":\n            warnings.warn(\n                \"cache_kv() is only supported for the HuggingFace backend. The KV cache for the vLLM backend is handled internally by vLLM. No operation performed.\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        elif self.backend == \"mock\":\n            pass\n        else:\n            raise RuntimeError(\n                f\"cache_kv() is not implemented for backend type {type(self.model)}\"\n            )\n</code></pre>"},{"location":"reference/hfppl/llms/#hfppl.llms.CachedCausalLM.vocab","title":"<code>vocab</code>  <code>property</code>","text":"<p>Legacy accessor for string vocabulary. Prefer using <code>.str_vocab</code> directly for access to the model's string vocabulary.</p>"},{"location":"reference/hfppl/llms/#hfppl.llms.CachedCausalLM.__init__","title":"<code>__init__(model)</code>","text":"<p>Create a <code>CachedCausalLM</code> from an <code>AsyncLM</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>AsyncLM</code> <p>an <code>AsyncLM</code> instance.</p> required Source code in <code>hfppl/llms.py</code> <pre><code>def __init__(self, model):\n    \"\"\"\n    Create a `CachedCausalLM` from an `AsyncLM`.\n\n    Args:\n        model (genlm_backend.llm.AsyncLM): an `AsyncLM` instance.\n    \"\"\"\n    if isinstance(model, AsyncVirtualLM):\n        self.backend = \"vllm\"\n    elif isinstance(model, AsyncTransformer):\n        self.backend = \"hf\"\n    elif isinstance(model, MockAsyncLM):\n        self.backend = \"mock\"\n    else:\n        raise ValueError(\n            f\"Unknown model type: {type(model)}. Must be one of [AsyncVirtualLM, AsyncTransformer, MockAsyncLM]\"\n        )\n\n    self.model = model\n    self.tokenizer = model.tokenizer\n    self.str_vocab = model.str_vocab\n    self.byte_vocab = model.byte_vocab\n    self.masks = Masks(self)\n</code></pre>"},{"location":"reference/hfppl/llms/#hfppl.llms.CachedCausalLM.cache_kv","title":"<code>cache_kv(prompt_tokens)</code>","text":"<p>Cache the key and value vectors for a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_tokens</code> <code>list[int]</code> <p>token ids for the prompt to cache.</p> required Source code in <code>hfppl/llms.py</code> <pre><code>def cache_kv(self, prompt_tokens):\n    \"\"\"Cache the key and value vectors for a prompt.\n\n    Args:\n        prompt_tokens (list[int]): token ids for the prompt to cache.\n    \"\"\"\n    if self.backend == \"hf\":\n        self.model.cache_kv(prompt_tokens)\n    elif self.backend == \"vllm\":\n        warnings.warn(\n            \"cache_kv() is only supported for the HuggingFace backend. The KV cache for the vLLM backend is handled internally by vLLM. No operation performed.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n    elif self.backend == \"mock\":\n        pass\n    else:\n        raise RuntimeError(\n            f\"cache_kv() is not implemented for backend type {type(self.model)}\"\n        )\n</code></pre>"},{"location":"reference/hfppl/llms/#hfppl.llms.CachedCausalLM.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the cache of log probabilities and key/value pairs.</p> <p>For HuggingFace backend: Clears both logprob cache and KV cache.</p> <p>For vLLM backend: Only clears logprob cache (KV cache is managed internally by vLLM).</p> Source code in <code>hfppl/llms.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the cache of log probabilities and key/value pairs.\n\n    For HuggingFace backend: Clears both logprob cache and KV cache.\n\n    For vLLM backend: Only clears logprob cache (KV cache is managed internally by vLLM).\n    \"\"\"\n    self.model.clear_cache()\n</code></pre>"},{"location":"reference/hfppl/llms/#hfppl.llms.CachedCausalLM.clear_kv_cache","title":"<code>clear_kv_cache()</code>","text":"<p>Clear any key and value vectors from the cache.</p> Source code in <code>hfppl/llms.py</code> <pre><code>def clear_kv_cache(self):\n    \"\"\"Clear any key and value vectors from the cache.\"\"\"\n    if self.backend == \"hf\":\n        self.model.clear_kv_cache()\n    elif self.backend == \"vllm\":\n        warnings.warn(\n            \"clear_kv_cache() is only supported for the HuggingFace backend. The KV cache for the vLLM backend is handled internally by vLLM. No operation performed.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n    elif self.backend == \"mock\":\n        pass\n    else:\n        raise RuntimeError(\n            f\"clear_kv_cache() is not implemented for backend type {type(self.model)}\"\n        )\n</code></pre>"},{"location":"reference/hfppl/llms/#hfppl.llms.CachedCausalLM.from_pretrained","title":"<code>from_pretrained(model_id, backend=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a CachedCausalLM from a HuggingFace model name.</p> <p>This is a convenience method that instantiates the underlying <code>AsyncLM</code> from a HuggingFace model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Name or path of the HuggingFace pretrained model to load.</p> required <code>backend</code> <code>str</code> <p><code>AsyncLM</code> backend to use: - 'vllm' to instantiate an <code>AsyncVirtualLM</code>; ideal for GPU usage - 'hf' for an <code>AsyncTransformer</code>; ideal for CPU usage - 'mock' for a <code>MockAsyncLM</code>; ideal for testing. Defaults to 'vllm' if CUDA is available, otherwise 'hf'.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>AsyncLM</code> constructor. See <code>AsyncLM</code> documentation.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>CachedCausalLM</code> <p>The hfppl-compatible interface to the <code>AsyncLM</code> model.</p> Source code in <code>hfppl/llms.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, model_id, backend=None, **kwargs):\n    \"\"\"Create a CachedCausalLM from a HuggingFace model name.\n\n    This is a convenience method that instantiates the underlying `AsyncLM` from a HuggingFace model name.\n\n    Args:\n        model_id (str): Name or path of the HuggingFace pretrained model to load.\n        backend (str, optional): `AsyncLM` backend to use:\n            - 'vllm' to instantiate an `AsyncVirtualLM`; ideal for GPU usage\n            - 'hf' for an `AsyncTransformer`; ideal for CPU usage\n            - 'mock' for a `MockAsyncLM`; ideal for testing.\n            Defaults to 'vllm' if CUDA is available, otherwise 'hf'.\n        **kwargs: Additional keyword arguments passed to the `AsyncLM` constructor.\n            See [`AsyncLM` documentation](https://probcomp.github.io/genlm-backend/reference/genlm_backend/llm/__init__/).\n\n    Returns:\n        CachedCausalLM: The hfppl-compatible interface to the `AsyncLM` model.\n    \"\"\"\n    backend = backend or (\n        \"vllm\" if (torch.cuda.is_available() and VLLM_AVAILABLE) else \"hf\"\n    )\n\n    if backend == \"vllm\":\n        if not VLLM_AVAILABLE:\n            raise ValueError(\n                \"vLLM backend requested but vLLM is not installed. \"\n                \"Please install vLLM with `pip install vllm`.\"\n            )\n        model_cls = AsyncVirtualLM\n    elif backend == \"hf\":\n        model_cls = AsyncTransformer\n    elif backend == \"mock\":\n        model_cls = MockAsyncLM\n    else:\n        raise ValueError(\n            f\"Unknown backend: {backend}. Must be one of ['vllm', 'hf', 'mock']\"\n        )\n\n    # Handle legacy auth_token parameter. The ability to pass in the auth_token should\n    # be removed in a future version since it is not supported by the vllm backend.\n    # Users should authenticate with the HuggingFace CLI.\n    auth_token = kwargs.pop(\"auth_token\", None)\n    if auth_token:\n        if backend == \"vllm\":\n            raise ValueError(\n                \"Explicitly passing auth_token is not compatible with the vLLM AsyncLM backend. \"\n                \"Authenticate using `huggingface-cli login` instead.\"\n            )\n\n        if \"hf_opts\" not in kwargs:\n            kwargs[\"hf_opts\"] = {}\n        kwargs[\"hf_opts\"][\"token\"] = auth_token\n\n        warnings.warn(\n            \"Passing auth_token directly is deprecated and will be removed in a future version. \"\n            \"Please authenticate using `huggingface-cli login` instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    load_in_8bit = kwargs.pop(\"load_in_8bit\", False)\n    if load_in_8bit:\n        if \"bitsandbytes_opts\" not in kwargs:\n            kwargs[\"bitsandbytes_opts\"] = {}\n        kwargs[\"bitsandbytes_opts\"][\"load_in_8bit\"] = True\n\n        warnings.warn(\n            \"load_in_8bit is deprecated and will be removed in a future version. \"\n            \"Please pass `bitsandbytes_opts` instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    model = model_cls.from_name(model_id, **kwargs)\n\n    return cls(model)\n</code></pre>"},{"location":"reference/hfppl/llms/#hfppl.llms.CachedCausalLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token. This version is asynchronous and support auto batching of concurrent requests; use with <code>await</code>.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>array</code> <p>a numpy array of length <code>len(str_vocab)</code> (equivalently <code>len(byte_vocab)</code>) with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>hfppl/llms.py</code> <pre><code>async def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token. This version is asynchronous and support auto batching of concurrent requests; use with `await`.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (numpy.array): a numpy array of length `len(str_vocab)` (equivalently `len(byte_vocab)`) with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    logprobs = await self.model.next_token_logprobs(token_ids)\n    return logprobs.float().cpu().numpy()\n</code></pre>"},{"location":"reference/hfppl/llms/#hfppl.llms.CachedCausalLM.next_token_logprobs_unbatched","title":"<code>next_token_logprobs_unbatched(token_ids)</code>","text":"<p>Request log probabilities of next token. Not asynchronous, and does not support auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>array</code> <p>a numpy array of length <code>len(str_vocab)</code> (equivalently <code>len(byte_vocab)</code>) with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>hfppl/llms.py</code> <pre><code>def next_token_logprobs_unbatched(self, token_ids):\n    \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (numpy.array): a numpy array of length `len(str_vocab)` (equivalently `len(byte_vocab)`) with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    return self.model.next_token_logprobs_sync(token_ids).float().cpu().numpy()\n</code></pre>"},{"location":"reference/hfppl/llms/#hfppl.llms.CachedCausalLM.reset_async_queries","title":"<code>reset_async_queries()</code>","text":"<p>Clear any pending language model queries from the queue.</p> Source code in <code>hfppl/llms.py</code> <pre><code>def reset_async_queries(self):\n    \"\"\"Clear any pending language model queries from the queue.\"\"\"\n    if self.backend == \"hf\":\n        self.model.reset_async_queries()\n    elif self.backend == \"vllm\":\n        warnings.warn(\n            \"reset_async_queries() is only supported for the HuggingFace backend. No operation performed.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n    elif self.backend == \"mock\":\n        pass\n    else:\n        raise RuntimeError(\n            f\"reset_async_queries() is not implemented for backend type {type(self.model)}\"\n        )\n</code></pre>"},{"location":"reference/hfppl/llms/#hfppl.llms.Masks","title":"<code>Masks</code>","text":"Source code in <code>hfppl/llms.py</code> <pre><code>class Masks:\n    def __init__(self, lm):\n        self.ALL_TOKENS = set(range(len(lm.str_vocab)))\n        self.STARTS_NEW_WORD = set(\n            i\n            for (i, v) in enumerate(lm.str_vocab)\n            if v[0] == \" \"\n            and len(v) &gt; 1\n            and v[1] not in string.whitespace\n            and v[1] not in string.punctuation\n        )\n        self.CONTINUES_CURRENT_WORD = set(\n            i\n            for (i, v) in enumerate(lm.str_vocab)\n            if all(c in \"'\" or c.isalpha() for c in v)\n        )\n        self.MID_PUNCTUATION = set(\n            i for (i, v) in enumerate(lm.str_vocab) if v in (\",\", \":\", \";\", \"-\", '\"')\n        )\n        self.END_PUNCTUATION = set(\n            i for (i, v) in enumerate(lm.str_vocab) if v in (\".\", \"!\", \"?\")\n        )\n        self.PUNCTUATION = self.MID_PUNCTUATION | self.END_PUNCTUATION\n        self.CONTAINS_WHITESPACE = set(\n            i\n            for (i, v) in enumerate(lm.str_vocab)\n            if any(c in string.whitespace for c in v)\n        )\n        self.EOS = set([lm.tokenizer.eos_token_id])\n\n        self.MAX_TOKEN_LENGTH = self.precompute_token_length_masks(lm)\n\n    def precompute_token_length_masks(self, lm):\n        \"\"\"Precompute masks for tokens of different lengths.\n\n        Each mask is a set of token ids that are of the given length or shorter.\"\"\"\n        max_token_length = max([len(t) for t in lm.str_vocab])\n\n        masks = defaultdict(lambda: self.ALL_TOKENS)\n        masks[0] = set([lm.tokenizer.eos_token_id])\n        for token_length in range(1, max_token_length + 1):\n            masks[token_length] = set(\n                i\n                for (i, v) in enumerate(lm.str_vocab)\n                if len(v) &lt;= token_length and i != lm.tokenizer.eos_token_id\n            )\n\n        return masks\n</code></pre>"},{"location":"reference/hfppl/llms/#hfppl.llms.Masks.precompute_token_length_masks","title":"<code>precompute_token_length_masks(lm)</code>","text":"<p>Precompute masks for tokens of different lengths.</p> <p>Each mask is a set of token ids that are of the given length or shorter.</p> Source code in <code>hfppl/llms.py</code> <pre><code>def precompute_token_length_masks(self, lm):\n    \"\"\"Precompute masks for tokens of different lengths.\n\n    Each mask is a set of token ids that are of the given length or shorter.\"\"\"\n    max_token_length = max([len(t) for t in lm.str_vocab])\n\n    masks = defaultdict(lambda: self.ALL_TOKENS)\n    masks[0] = set([lm.tokenizer.eos_token_id])\n    for token_length in range(1, max_token_length + 1):\n        masks[token_length] = set(\n            i\n            for (i, v) in enumerate(lm.str_vocab)\n            if len(v) &lt;= token_length and i != lm.tokenizer.eos_token_id\n        )\n\n    return masks\n</code></pre>"},{"location":"reference/hfppl/llms/#hfppl.llms.Token","title":"<code>Token</code>","text":"<p>Class representing a token.</p> <p>Attributes:</p> Name Type Description <code>lm</code> <code>CachedCausalLM</code> <p>the language model for which this is a Token.</p> <code>token_id</code> <code>int</code> <p>the integer token id (an index into the vocabulary).</p> <code>token_str</code> <code>str</code> <p>a string, which the token represents\u2014equal to <code>lm.str_vocab[token_id]</code>.</p> Source code in <code>hfppl/llms.py</code> <pre><code>class Token:\n    \"\"\"Class representing a token.\n\n    Attributes:\n        lm (hfppl.llms.CachedCausalLM): the language model for which this is a Token.\n        token_id (int): the integer token id (an index into the vocabulary).\n        token_str (str): a string, which the token represents\u2014equal to `lm.str_vocab[token_id]`.\n    \"\"\"\n\n    def __init__(self, lm, token_id, token_str):\n        self.lm = lm\n        self.token_id = token_id\n        self.token_str = token_str\n\n    # Adding tokens\n    def __add__(self, other):\n        s = TokenSequence(self.lm, [self.token_id])\n        s += other\n        return s\n\n    def __radd__(self, other):\n        s = TokenSequence(self.lm, [self.token_id])\n        return other + s\n\n    # Support checking for EOS\n    def __eq__(self, other):\n        if isinstance(other, Token):\n            return self.lm is other.lm and self.token_id == other.token_id\n        elif isinstance(other, int):\n            return self.token_id == other\n        else:\n            return self.token_str == other\n\n    def __int__(self):\n        return self.token_id\n\n    def __str__(self):\n        return self.token_str\n\n    def __repr__(self):\n        return f\"&lt;{self.token_str}|{self.token_id}&gt;\"\n</code></pre>"},{"location":"reference/hfppl/llms/#hfppl.llms.TokenSequence","title":"<code>TokenSequence</code>","text":"<p>A sequence of tokens.</p> <p>Supports addition (via <code>+</code> or mutating <code>+=</code>) with:</p> <ul> <li>other <code>TokenSequence</code> instances (concatenation)</li> <li>individual tokens, represented as integers or <code>Token</code> instances</li> <li>strings, which are tokenized by <code>lm.tokenizer</code></li> </ul> <p>Attributes:</p> Name Type Description <code>lm</code> <code>CachedCausalLM</code> <p>the language model whose vocabulary the tokens come from.</p> <code>seq</code> <code>list[Token]</code> <p>the sequence of tokens.</p> Source code in <code>hfppl/llms.py</code> <pre><code>class TokenSequence:\n    \"\"\"A sequence of tokens.\n\n    Supports addition (via `+` or mutating `+=`) with:\n\n    * other `TokenSequence` instances (concatenation)\n    * individual tokens, represented as integers or `Token` instances\n    * strings, which are tokenized by `lm.tokenizer`\n\n    Attributes:\n        lm (hfppl.llms.CachedCausalLM): the language model whose vocabulary the tokens come from.\n        seq (list[hfppl.llms.Token]): the sequence of tokens.\"\"\"\n\n    def __init__(self, lm, seq=None):\n        \"\"\"Create a `TokenSequence` from a language model and a sequence.\n\n        Args:\n            lm (hfppl.llms.CachedCausalLM): the language model whose vocabulary the tokens come from.\n            seq (str | list[int]): the sequence of token ids, or a string which will be automatically tokenized. Defaults to the singleton sequence containing a bos token.\n        \"\"\"\n        self.lm = lm\n        if seq is None:\n            self.seq = [lm.tokenizer.bos_token_id]\n        elif isinstance(seq, str):\n            self.seq = self.lm.tokenizer.encode(seq)\n        else:\n            self.seq = seq\n\n    def __str__(self):\n        return self.lm.tokenizer.decode(self.seq)\n\n    def __iadd__(self, other):\n        if isinstance(other, Token):\n            assert other.lm is self.lm\n            self.seq.append(other.token_id)\n        elif isinstance(other, TokenSequence):\n            assert other.lm is self.lm\n            self.seq.extend(other.seq)\n        elif isinstance(other, str):\n            self.seq.extend(self.lm.tokenizer.encode(other, add_special_tokens=False))\n        elif isinstance(other, int):\n            self.seq.append(other)\n        else:\n            raise RuntimeError(f\"Addition not supported on {type(other)}\")\n        return self\n\n    def __radd__(self, other):\n        if isinstance(other, Token):\n            assert other.lm is self.lm\n            return TokenSequence(self.lm, [other.token_id, *self.seq])\n        elif isinstance(other, TokenSequence):\n            assert other.lm is self.lm\n            return TokenSequence(self.lm, other.seq + self.seq)\n        elif isinstance(other, str):\n            return TokenSequence(\n                self.lm,\n                self.lm.tokenizer.encode(other, add_special_tokens=False) + self.seq,\n            )\n        elif isinstance(other, int):\n            return TokenSequence(self.lm, [other, *self.seq])\n        else:\n            raise RuntimeError(f\"Addition not supported on {type(other)}\")\n\n    def __add__(self, other):\n        s = TokenSequence(self.lm, self.seq)\n        s += other\n        return s\n</code></pre>"},{"location":"reference/hfppl/llms/#hfppl.llms.TokenSequence.__init__","title":"<code>__init__(lm, seq=None)</code>","text":"<p>Create a <code>TokenSequence</code> from a language model and a sequence.</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <code>CachedCausalLM</code> <p>the language model whose vocabulary the tokens come from.</p> required <code>seq</code> <code>str | list[int]</code> <p>the sequence of token ids, or a string which will be automatically tokenized. Defaults to the singleton sequence containing a bos token.</p> <code>None</code> Source code in <code>hfppl/llms.py</code> <pre><code>def __init__(self, lm, seq=None):\n    \"\"\"Create a `TokenSequence` from a language model and a sequence.\n\n    Args:\n        lm (hfppl.llms.CachedCausalLM): the language model whose vocabulary the tokens come from.\n        seq (str | list[int]): the sequence of token ids, or a string which will be automatically tokenized. Defaults to the singleton sequence containing a bos token.\n    \"\"\"\n    self.lm = lm\n    if seq is None:\n        self.seq = [lm.tokenizer.bos_token_id]\n    elif isinstance(seq, str):\n        self.seq = self.lm.tokenizer.encode(seq)\n    else:\n        self.seq = seq\n</code></pre>"},{"location":"reference/hfppl/modeling/","title":"modeling","text":""},{"location":"reference/hfppl/modeling/#hfppl.modeling.Model","title":"<code>Model</code>","text":"<p>Base class for all LLaMPPL models.</p> <p>Your models should subclass this class. Minimally, you should provide an <code>__init__</code> method that calls <code>super().__init__(self)</code>, and a <code>step</code> method.</p> Source code in <code>hfppl/modeling.py</code> <pre><code>class Model:\n    \"\"\"Base class for all LLaMPPL models.\n\n    Your models should subclass this class. Minimally, you should provide an `__init__` method\n    that calls `super().__init__(self)`, and a `step` method.\n    \"\"\"\n\n    def __init__(self):\n        self.weight = 0.0\n        self.finished = False\n        self.mode = \"sample\"\n        self.beam_idx = 0\n        self.force_eos = False\n        self.twist_amount = 0.0\n\n    def reset(self):\n        self.weight = 0.0\n        self.finished = False\n        self.mode = \"sample\"\n        self.beam_idx = 0\n        self.force_eos = False\n        self.twist_amount = 0.0\n\n    def immutable_properties(self):\n        \"\"\"Return a `set[str]` of properties that LLaMPPL may assume do not change during execution of `step`.\n        This set is empty by default but can be overridden by subclasses to speed up inference.\n\n        Returns:\n            properties (set[str]): a set of immutable property names\"\"\"\n        return set()\n\n    def __deepcopy__(self, memo):\n        cpy = type(self).__new__(type(self))\n        immutable = self.immutable_properties()\n\n        for k, v in self.__dict__.items():\n            if k in immutable:\n                setattr(cpy, k, v)\n            else:\n                setattr(cpy, k, copy.deepcopy(v, memo))\n\n        return cpy\n\n    def twist(self, amt):\n        \"\"\"Multiply this particle's weight by `exp(amt)`, but divide it back out before the next `step`.\n\n        Use this method to provide heuristic guidance about whether a particle is \"on the right track\"\n        without changing the ultimate target distribution.\n\n        Args:\n            amt: the logarithm of the amount by which to (temporarily) multiply this particle's weight.\n        \"\"\"\n        self.twist_amount += amt\n        self.score(amt)\n\n    def untwist(self):\n        self.score(-self.twist_amount)\n        self.twist_amount = 0.0\n\n    def finish(self):\n        self.untwist()\n        self.finished = True\n\n    def done_stepping(self):\n        return self.finished\n\n    async def step(self):\n        \"\"\"Defines the computation performed in each step of the model.\n\n        All subclasses should override this method.\"\"\"\n\n        if not self.done_stepping():\n            raise NotImplementedError(\"Model.step() must be implemented by subclasses\")\n\n    def __str__(self):\n        return \"Particle\"\n\n    async def start(self):\n        pass\n\n    def score(self, score):\n        \"\"\"Multiply this particle's weight by `exp(score)`.\n\n        The `score` method is a low-level way to change the target distribution.\n        For many use cases, it is sufficient to use `sample`, `observe`, `condition`,\n        and `twist`, all of which are implemented in terms of `score`.\n\n        Args:\n            score: logarithm of the amount by which the particle's weight should be multiplied.\n        \"\"\"\n        self.weight += score\n\n    def condition(self, b):\n        \"\"\"Constrain a given Boolean expression to be `True`.\n\n        If the condition is False, the particle's weight is set to zero and `self.finish()`\n        is called, so that no further `step` calls are made.\n\n        Args:\n            b: the Boolean expression whose value is constrained to be True.\n        \"\"\"\n        if not b:\n            self.score(float(\"-inf\"))\n            self.finish()\n\n    async def intervene(self, dist, x):\n        \"\"\"Force the distribution to take on the value `x`, but do not _condition_ on this result.\n\n        This is useful primarily with distributions that have side effects (e.g., modifying some state).\n        For example, a model with the code\n\n        ```python\n        token_1 = await self.sample(self.stateful_lm.next_token())\n        await self.observe(self.stateful_lm.next_token(), token_2)\n        ```\n\n        encodes a posterior inference problem, to find `token_1` values that *likely preceded* `token_2`. By contrast,\n\n        ```python\n        token_1 = await self.sample(stateful_lm.next_token())\n        await self.intervene(self.stateful_lm.next_token(), token_2)\n        ```\n\n        encodes a much easier task: freely generate `token_1` and then force-feed `token_2` as the following token.\n\n        Args:\n            dist (hfppl.distributions.distribution.Distribution): the distribution on which to intervene.\n            x: the value to intervene with.\n        \"\"\"\n        await dist.log_prob(x)\n        return x\n\n    async def observe(self, dist, x):\n        \"\"\"Condition the model on the value `x` being sampled from the distribution `dist`.\n\n        For discrete distributions `dist`, `await self.observe(dist, x)` specifies the same constraint as\n        ```\n        val = await self.sample(dist)\n        self.condition(val == x)\n        ```\n        but can be much more efficient.\n\n        Args:\n            dist: a `Distribution` object from which to observe\n            x: the value observed from `dist`\n        \"\"\"\n        p = await dist.log_prob(x)\n        self.score(p)\n        return x\n\n    async def sample(self, dist, proposal=None):\n        \"\"\"Extend the model with a sample from a given `Distribution`, with support for autobatching.\n        If specified, the Distribution `proposal` is used during inference to generate informed hypotheses.\n\n        Args:\n            dist: the `Distribution` object from which to sample\n            proposal: if provided, inference algorithms will use this `Distribution` object to generate proposed samples, rather than `dist`.\n              However, importance weights will be adjusted so that the target posterior is independent of the proposal.\n\n        Returns:\n            value: the value sampled from the distribution.\n        \"\"\"\n        # Special logic for beam search\n        # if self.mode == \"beam\":\n        #     d = dist if proposal is None else proposal\n        #     x, w = d.argmax(self.beam_idx)\n        #     if proposal is not None:\n        #         self.score(dist.log_prob(x))\n        #     else:\n        #         self.score(w)\n        #     return x\n\n        if proposal is None:\n            x, _ = await dist.sample()\n            return x\n        else:\n            x, q = await proposal.sample()\n            p = await dist.log_prob(x)\n            self.score(p - q)\n            return x\n\n    async def call(self, submodel):\n        return await submodel.run_with_parent(self)\n\n    def string_for_serialization(self):\n        \"\"\"Return a string representation of the particle for serialization purposes.\n\n        Returns:\n            str: a string representation of the particle.\n        \"\"\"\n        return str(self)\n</code></pre>"},{"location":"reference/hfppl/modeling/#hfppl.modeling.Model.condition","title":"<code>condition(b)</code>","text":"<p>Constrain a given Boolean expression to be <code>True</code>.</p> <p>If the condition is False, the particle's weight is set to zero and <code>self.finish()</code> is called, so that no further <code>step</code> calls are made.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <p>the Boolean expression whose value is constrained to be True.</p> required Source code in <code>hfppl/modeling.py</code> <pre><code>def condition(self, b):\n    \"\"\"Constrain a given Boolean expression to be `True`.\n\n    If the condition is False, the particle's weight is set to zero and `self.finish()`\n    is called, so that no further `step` calls are made.\n\n    Args:\n        b: the Boolean expression whose value is constrained to be True.\n    \"\"\"\n    if not b:\n        self.score(float(\"-inf\"))\n        self.finish()\n</code></pre>"},{"location":"reference/hfppl/modeling/#hfppl.modeling.Model.immutable_properties","title":"<code>immutable_properties()</code>","text":"<p>Return a <code>set[str]</code> of properties that LLaMPPL may assume do not change during execution of <code>step</code>. This set is empty by default but can be overridden by subclasses to speed up inference.</p> <p>Returns:</p> Name Type Description <code>properties</code> <code>set[str]</code> <p>a set of immutable property names</p> Source code in <code>hfppl/modeling.py</code> <pre><code>def immutable_properties(self):\n    \"\"\"Return a `set[str]` of properties that LLaMPPL may assume do not change during execution of `step`.\n    This set is empty by default but can be overridden by subclasses to speed up inference.\n\n    Returns:\n        properties (set[str]): a set of immutable property names\"\"\"\n    return set()\n</code></pre>"},{"location":"reference/hfppl/modeling/#hfppl.modeling.Model.intervene","title":"<code>intervene(dist, x)</code>  <code>async</code>","text":"<p>Force the distribution to take on the value <code>x</code>, but do not condition on this result.</p> <p>This is useful primarily with distributions that have side effects (e.g., modifying some state). For example, a model with the code</p> <pre><code>token_1 = await self.sample(self.stateful_lm.next_token())\nawait self.observe(self.stateful_lm.next_token(), token_2)\n</code></pre> <p>encodes a posterior inference problem, to find <code>token_1</code> values that likely preceded <code>token_2</code>. By contrast,</p> <pre><code>token_1 = await self.sample(stateful_lm.next_token())\nawait self.intervene(self.stateful_lm.next_token(), token_2)\n</code></pre> <p>encodes a much easier task: freely generate <code>token_1</code> and then force-feed <code>token_2</code> as the following token.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>Distribution</code> <p>the distribution on which to intervene.</p> required <code>x</code> <p>the value to intervene with.</p> required Source code in <code>hfppl/modeling.py</code> <pre><code>async def intervene(self, dist, x):\n    \"\"\"Force the distribution to take on the value `x`, but do not _condition_ on this result.\n\n    This is useful primarily with distributions that have side effects (e.g., modifying some state).\n    For example, a model with the code\n\n    ```python\n    token_1 = await self.sample(self.stateful_lm.next_token())\n    await self.observe(self.stateful_lm.next_token(), token_2)\n    ```\n\n    encodes a posterior inference problem, to find `token_1` values that *likely preceded* `token_2`. By contrast,\n\n    ```python\n    token_1 = await self.sample(stateful_lm.next_token())\n    await self.intervene(self.stateful_lm.next_token(), token_2)\n    ```\n\n    encodes a much easier task: freely generate `token_1` and then force-feed `token_2` as the following token.\n\n    Args:\n        dist (hfppl.distributions.distribution.Distribution): the distribution on which to intervene.\n        x: the value to intervene with.\n    \"\"\"\n    await dist.log_prob(x)\n    return x\n</code></pre>"},{"location":"reference/hfppl/modeling/#hfppl.modeling.Model.observe","title":"<code>observe(dist, x)</code>  <code>async</code>","text":"<p>Condition the model on the value <code>x</code> being sampled from the distribution <code>dist</code>.</p> <p>For discrete distributions <code>dist</code>, <code>await self.observe(dist, x)</code> specifies the same constraint as <pre><code>val = await self.sample(dist)\nself.condition(val == x)\n</code></pre> but can be much more efficient.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <p>a <code>Distribution</code> object from which to observe</p> required <code>x</code> <p>the value observed from <code>dist</code></p> required Source code in <code>hfppl/modeling.py</code> <pre><code>async def observe(self, dist, x):\n    \"\"\"Condition the model on the value `x` being sampled from the distribution `dist`.\n\n    For discrete distributions `dist`, `await self.observe(dist, x)` specifies the same constraint as\n    ```\n    val = await self.sample(dist)\n    self.condition(val == x)\n    ```\n    but can be much more efficient.\n\n    Args:\n        dist: a `Distribution` object from which to observe\n        x: the value observed from `dist`\n    \"\"\"\n    p = await dist.log_prob(x)\n    self.score(p)\n    return x\n</code></pre>"},{"location":"reference/hfppl/modeling/#hfppl.modeling.Model.sample","title":"<code>sample(dist, proposal=None)</code>  <code>async</code>","text":"<p>Extend the model with a sample from a given <code>Distribution</code>, with support for autobatching. If specified, the Distribution <code>proposal</code> is used during inference to generate informed hypotheses.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <p>the <code>Distribution</code> object from which to sample</p> required <code>proposal</code> <p>if provided, inference algorithms will use this <code>Distribution</code> object to generate proposed samples, rather than <code>dist</code>. However, importance weights will be adjusted so that the target posterior is independent of the proposal.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>value</code> <p>the value sampled from the distribution.</p> Source code in <code>hfppl/modeling.py</code> <pre><code>async def sample(self, dist, proposal=None):\n    \"\"\"Extend the model with a sample from a given `Distribution`, with support for autobatching.\n    If specified, the Distribution `proposal` is used during inference to generate informed hypotheses.\n\n    Args:\n        dist: the `Distribution` object from which to sample\n        proposal: if provided, inference algorithms will use this `Distribution` object to generate proposed samples, rather than `dist`.\n          However, importance weights will be adjusted so that the target posterior is independent of the proposal.\n\n    Returns:\n        value: the value sampled from the distribution.\n    \"\"\"\n    # Special logic for beam search\n    # if self.mode == \"beam\":\n    #     d = dist if proposal is None else proposal\n    #     x, w = d.argmax(self.beam_idx)\n    #     if proposal is not None:\n    #         self.score(dist.log_prob(x))\n    #     else:\n    #         self.score(w)\n    #     return x\n\n    if proposal is None:\n        x, _ = await dist.sample()\n        return x\n    else:\n        x, q = await proposal.sample()\n        p = await dist.log_prob(x)\n        self.score(p - q)\n        return x\n</code></pre>"},{"location":"reference/hfppl/modeling/#hfppl.modeling.Model.score","title":"<code>score(score)</code>","text":"<p>Multiply this particle's weight by <code>exp(score)</code>.</p> <p>The <code>score</code> method is a low-level way to change the target distribution. For many use cases, it is sufficient to use <code>sample</code>, <code>observe</code>, <code>condition</code>, and <code>twist</code>, all of which are implemented in terms of <code>score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <p>logarithm of the amount by which the particle's weight should be multiplied.</p> required Source code in <code>hfppl/modeling.py</code> <pre><code>def score(self, score):\n    \"\"\"Multiply this particle's weight by `exp(score)`.\n\n    The `score` method is a low-level way to change the target distribution.\n    For many use cases, it is sufficient to use `sample`, `observe`, `condition`,\n    and `twist`, all of which are implemented in terms of `score`.\n\n    Args:\n        score: logarithm of the amount by which the particle's weight should be multiplied.\n    \"\"\"\n    self.weight += score\n</code></pre>"},{"location":"reference/hfppl/modeling/#hfppl.modeling.Model.step","title":"<code>step()</code>  <code>async</code>","text":"<p>Defines the computation performed in each step of the model.</p> <p>All subclasses should override this method.</p> Source code in <code>hfppl/modeling.py</code> <pre><code>async def step(self):\n    \"\"\"Defines the computation performed in each step of the model.\n\n    All subclasses should override this method.\"\"\"\n\n    if not self.done_stepping():\n        raise NotImplementedError(\"Model.step() must be implemented by subclasses\")\n</code></pre>"},{"location":"reference/hfppl/modeling/#hfppl.modeling.Model.string_for_serialization","title":"<code>string_for_serialization()</code>","text":"<p>Return a string representation of the particle for serialization purposes.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>a string representation of the particle.</p> Source code in <code>hfppl/modeling.py</code> <pre><code>def string_for_serialization(self):\n    \"\"\"Return a string representation of the particle for serialization purposes.\n\n    Returns:\n        str: a string representation of the particle.\n    \"\"\"\n    return str(self)\n</code></pre>"},{"location":"reference/hfppl/modeling/#hfppl.modeling.Model.twist","title":"<code>twist(amt)</code>","text":"<p>Multiply this particle's weight by <code>exp(amt)</code>, but divide it back out before the next <code>step</code>.</p> <p>Use this method to provide heuristic guidance about whether a particle is \"on the right track\" without changing the ultimate target distribution.</p> <p>Parameters:</p> Name Type Description Default <code>amt</code> <p>the logarithm of the amount by which to (temporarily) multiply this particle's weight.</p> required Source code in <code>hfppl/modeling.py</code> <pre><code>def twist(self, amt):\n    \"\"\"Multiply this particle's weight by `exp(amt)`, but divide it back out before the next `step`.\n\n    Use this method to provide heuristic guidance about whether a particle is \"on the right track\"\n    without changing the ultimate target distribution.\n\n    Args:\n        amt: the logarithm of the amount by which to (temporarily) multiply this particle's weight.\n    \"\"\"\n    self.twist_amount += amt\n    self.score(amt)\n</code></pre>"},{"location":"reference/hfppl/modeling/#hfppl.modeling.submodel","title":"<code>submodel(f)</code>","text":"<p>Decorator to create a SubModel implementation from an async function.</p> <p>For example:</p> <pre><code>@submodel\nasync def sample_two_tokens(self, context):\n    token1 = await self.sample(context.next_token())\n    token2 = await self.sample(context.next_token())\n    return token1, token2\n</code></pre> <p>This SubModel can then be used from another model or submodel, using the syntax <code>await self.call(sample_two_tokens(context))</code>.</p> Source code in <code>hfppl/modeling.py</code> <pre><code>def submodel(f):\n    \"\"\"Decorator to create a SubModel implementation from an async function.\n\n    For example:\n\n    ```python\n    @submodel\n    async def sample_two_tokens(self, context):\n        token1 = await self.sample(context.next_token())\n        token2 = await self.sample(context.next_token())\n        return token1, token2\n    ```\n\n    This SubModel can then be used from another model or submodel, using the syntax `await self.call(sample_two_tokens(context))`.\n    \"\"\"\n\n    @functools.wraps(f, updated=())  # unclear if this is the best way to do it\n    class SubModelImpl(SubModel):\n        def __init__(self, *args, **kwargs):\n            super().__init__()\n            self.args = args\n            self.kwargs = kwargs\n\n        async def forward(self):\n            return await f(self, *self.args, **self.kwargs)\n\n    return SubModelImpl\n</code></pre>"},{"location":"reference/hfppl/util/","title":"util","text":"<p>Utility functions</p>"},{"location":"reference/hfppl/util/#hfppl.util.log_softmax","title":"<code>log_softmax(nums)</code>","text":"<p>Compute log(softmax(nums)).</p> <p>Parameters:</p> Name Type Description Default <code>nums</code> <p>a vector or numpy array of unnormalized log probabilities.</p> required <p>Returns:</p> Type Description <p>np.array: an array of log (normalized) probabilities.</p> Source code in <code>hfppl/util.py</code> <pre><code>def log_softmax(nums):\n    \"\"\"Compute log(softmax(nums)).\n\n    Args:\n        nums: a vector or numpy array of unnormalized log probabilities.\n\n    Returns:\n        np.array: an array of log (normalized) probabilities.\n    \"\"\"\n    return nums - logsumexp(nums)\n</code></pre>"},{"location":"reference/hfppl/distributions/__init__/","title":"distributions","text":"<p>Exposes distributions for use with <code>sample</code>, <code>observe</code>, and <code>intervene</code> methods in LLaMPPL models.</p> <p>Currently supported distributions:</p> <ul> <li><code>Bernoulli(p: float) -&gt; bool</code></li> <li><code>Geometric(p: float) -&gt; int</code></li> <li><code>LogCategorical(logits: array) -&gt; int</code></li> <li><code>TokenCategorical(lm: hfppl.llms.CachedCausalLM, logits: array) -&gt; hfppl.llms.Token</code></li> <li><code>Transformer(lm: hfppl.llms.CachedCausalLM) -&gt; hfppl.llms.Token</code></li> <li><code>LMContext(lm: hfppl.llms.CachedCausalLM, prompt: list[int]).next_token() -&gt; hfppl.llms.Token</code></li> <li><code>LMContext(lm: hfppl.llms.CachedCausalLM, prompt: list[int]).mask_dist(mask: set[int]) -&gt; bool</code></li> </ul>"},{"location":"reference/hfppl/distributions/bernoulli/","title":"bernoulli","text":""},{"location":"reference/hfppl/distributions/bernoulli/#hfppl.distributions.bernoulli.Bernoulli","title":"<code>Bernoulli</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A Bernoulli distribution.</p> Source code in <code>hfppl/distributions/bernoulli.py</code> <pre><code>class Bernoulli(Distribution):\n    \"\"\"A Bernoulli distribution.\"\"\"\n\n    def __init__(self, p):\n        \"\"\"Create a Bernoulli distribution.\n\n        Args:\n            p: the probability-of-True for the Bernoulli distribution.\n        \"\"\"\n        self.p = p\n\n    async def sample(self):\n        b = np.random.rand() &lt; self.p\n        return (b, await self.log_prob(b))\n\n    async def log_prob(self, value):\n        return np.log(self.p) if value else np.log1p(-self.p)\n\n    async def argmax(self, idx):\n        return (self.p &gt; 0.5) if idx == 0 else (self.p &lt; 0.5)\n</code></pre>"},{"location":"reference/hfppl/distributions/bernoulli/#hfppl.distributions.bernoulli.Bernoulli.__init__","title":"<code>__init__(p)</code>","text":"<p>Create a Bernoulli distribution.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <p>the probability-of-True for the Bernoulli distribution.</p> required Source code in <code>hfppl/distributions/bernoulli.py</code> <pre><code>def __init__(self, p):\n    \"\"\"Create a Bernoulli distribution.\n\n    Args:\n        p: the probability-of-True for the Bernoulli distribution.\n    \"\"\"\n    self.p = p\n</code></pre>"},{"location":"reference/hfppl/distributions/distribution/","title":"distribution","text":""},{"location":"reference/hfppl/distributions/distribution/#hfppl.distributions.distribution.Distribution","title":"<code>Distribution</code>","text":"<p>Abstract base class for a distribution.</p> Source code in <code>hfppl/distributions/distribution.py</code> <pre><code>class Distribution:\n    \"\"\"Abstract base class for a distribution.\"\"\"\n\n    async def sample(self):\n        \"\"\"Generate a random sample from the distribution.\n\n        Returns:\n            x: a value randomly sampled from the distribution.\"\"\"\n        raise NotImplementedError()\n\n    async def log_prob(self, x):\n        \"\"\"Compute the log probability of a value under this distribution,\n        or the log probability density if the distribution is continuous.\n\n        Args:\n            x: the point at which to evaluate the log probability.\n        Returns:\n            logprob (float): the log probability of `x`.\"\"\"\n        raise NotImplementedError()\n\n    async def argmax(self, n):\n        \"\"\"Return the nth most probable outcome under this distribution (assuming this is a discrete distribution).\n\n        Args:\n            n (int): which value to return to, indexed from most probable (n=0) to least probable (n=|support|).\n        Returns:\n            x: the nth most probable outcome from this distribution.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/hfppl/distributions/distribution/#hfppl.distributions.distribution.Distribution.argmax","title":"<code>argmax(n)</code>  <code>async</code>","text":"<p>Return the nth most probable outcome under this distribution (assuming this is a discrete distribution).</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>which value to return to, indexed from most probable (n=0) to least probable (n=|support|).</p> required <p>Returns:     x: the nth most probable outcome from this distribution.</p> Source code in <code>hfppl/distributions/distribution.py</code> <pre><code>async def argmax(self, n):\n    \"\"\"Return the nth most probable outcome under this distribution (assuming this is a discrete distribution).\n\n    Args:\n        n (int): which value to return to, indexed from most probable (n=0) to least probable (n=|support|).\n    Returns:\n        x: the nth most probable outcome from this distribution.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/hfppl/distributions/distribution/#hfppl.distributions.distribution.Distribution.log_prob","title":"<code>log_prob(x)</code>  <code>async</code>","text":"<p>Compute the log probability of a value under this distribution, or the log probability density if the distribution is continuous.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>the point at which to evaluate the log probability.</p> required <p>Returns:     logprob (float): the log probability of <code>x</code>.</p> Source code in <code>hfppl/distributions/distribution.py</code> <pre><code>async def log_prob(self, x):\n    \"\"\"Compute the log probability of a value under this distribution,\n    or the log probability density if the distribution is continuous.\n\n    Args:\n        x: the point at which to evaluate the log probability.\n    Returns:\n        logprob (float): the log probability of `x`.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/hfppl/distributions/distribution/#hfppl.distributions.distribution.Distribution.sample","title":"<code>sample()</code>  <code>async</code>","text":"<p>Generate a random sample from the distribution.</p> <p>Returns:</p> Name Type Description <code>x</code> <p>a value randomly sampled from the distribution.</p> Source code in <code>hfppl/distributions/distribution.py</code> <pre><code>async def sample(self):\n    \"\"\"Generate a random sample from the distribution.\n\n    Returns:\n        x: a value randomly sampled from the distribution.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/hfppl/distributions/geometric/","title":"geometric","text":""},{"location":"reference/hfppl/distributions/geometric/#hfppl.distributions.geometric.Geometric","title":"<code>Geometric</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A Geometric distribution.</p> Source code in <code>hfppl/distributions/geometric.py</code> <pre><code>class Geometric(Distribution):\n    \"\"\"A Geometric distribution.\"\"\"\n\n    def __init__(self, p):\n        \"\"\"Create a Geometric distribution.\n\n        Args:\n            p: the rate of the Geometric distribution.\n        \"\"\"\n        self.p = p\n\n    async def sample(self):\n        n = np.random.geometric(self.p)\n        return n, await self.log_prob(n)\n\n    async def log_prob(self, value):\n        return np.log(self.p) + np.log(1 - self.p) * (value - 1)\n\n    async def argmax(self, idx):\n        return idx - 1  # Most likely outcome is 0, then 1, etc.\n</code></pre>"},{"location":"reference/hfppl/distributions/geometric/#hfppl.distributions.geometric.Geometric.__init__","title":"<code>__init__(p)</code>","text":"<p>Create a Geometric distribution.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <p>the rate of the Geometric distribution.</p> required Source code in <code>hfppl/distributions/geometric.py</code> <pre><code>def __init__(self, p):\n    \"\"\"Create a Geometric distribution.\n\n    Args:\n        p: the rate of the Geometric distribution.\n    \"\"\"\n    self.p = p\n</code></pre>"},{"location":"reference/hfppl/distributions/lmcontext/","title":"lmcontext","text":""},{"location":"reference/hfppl/distributions/lmcontext/#hfppl.distributions.lmcontext.LMContext","title":"<code>LMContext</code>","text":"<p>Represents a generation-in-progress from a language model.</p> <p>The state tracks two pieces of information:</p> <ul> <li>A sequence of tokens \u2014 the ever-growing context for the language model.</li> <li>A current mask \u2014 a set of tokens that have not yet been ruled out as the next token.</li> </ul> <p>Storing a mask enables sub-token generation: models can use <code>LMContext</code> to sample the next token in stages, first deciding, e.g., whether to use an upper-case or lower-case first letter, and only later deciding which upper-case or lower-case token to generate.</p> <p>The state of a <code>LMContext</code> can be advanced in two ways:</p> <ol> <li>Sampling, observing, or intervening the <code>next_token()</code> distribution. This causes a token to be added to the growing sequence of tokens. Supports auto-batching.</li> <li>Sampling, observing, or intervening the <code>mask_dist(mask)</code> distribution for a given mask (set of token ids). This changes the current mask.</li> </ol> <p>Attributes:</p> Name Type Description <code>lm</code> <code>CachedCausalLM</code> <p>the language model for which this is a context</p> <code>tokens</code> <code>list[int]</code> <p>the underlying sequence of tokens, including prompt, in this context</p> <code>next_token_logprobs</code> <code>array</code> <p>numpy array holding the log probabilities for the next token. Unlike the log probabilities reported by <code>CachedCausalLM.next_token_logprobs</code>, these probabilities are rescaled for this <code>LMContext</code>'s temperature parameter, and for any active masks. This vector is managed by the <code>LMContext</code> object internally; do not mutate.</p> <code>temp</code> <code>float</code> <p>temeprature for next-token distribution (0 &lt; temp &lt; float('inf'))</p> <code>model_mask</code> <code>set[int]</code> <p>set of tokens that have not been ruled out as the next token. This mask is managed by the <code>LMContext</code> object internally; do not mutate.</p> <code>show_prompt</code> <code>bool</code> <p>controls whether the string representation of this <code>LMContext</code> includes the initial prompt or not. Defaults to <code>False</code>.</p> Source code in <code>hfppl/distributions/lmcontext.py</code> <pre><code>class LMContext:\n    \"\"\"Represents a generation-in-progress from a language model.\n\n    The state tracks two pieces of information:\n\n    * A sequence of tokens \u2014 the ever-growing context for the language model.\n    * A *current mask* \u2014 a set of tokens that have not yet been ruled out as the next token.\n\n    Storing a mask enables _sub-token_ generation: models can use `LMContext` to sample\n    the next token in _stages_, first deciding, e.g., whether to use an upper-case or lower-case\n    first letter, and only later deciding which upper-case or lower-case token to generate.\n\n    The state of a `LMContext` can be advanced in two ways:\n\n    1. Sampling, observing, or intervening the `next_token()` distribution. This causes a token\n    to be added to the growing sequence of tokens. Supports auto-batching.\n    2. Sampling, observing, or intervening the `mask_dist(mask)` distribution for a given mask (set of\n    token ids). This changes the current mask.\n\n    Attributes:\n        lm (hfppl.llms.CachedCausalLM): the language model for which this is a context\n        tokens (list[int]): the underlying sequence of tokens, including prompt, in this context\n        next_token_logprobs (numpy.array): numpy array holding the log probabilities for the next token. Unlike the log probabilities reported by `CachedCausalLM.next_token_logprobs`, these probabilities are rescaled for this `LMContext`'s temperature parameter, and for any active masks. This vector is managed by the `LMContext` object internally; do not mutate.\n        temp (float): temeprature for next-token distribution (0 &lt; temp &lt; float('inf'))\n        model_mask (set[int]): set of tokens that have not been ruled out as the next token. This mask is managed by the `LMContext` object internally; do not mutate.\n        show_prompt (bool): controls whether the string representation of this `LMContext` includes the initial prompt or not. Defaults to `False`.\n    \"\"\"\n\n    def __init__(self, lm, prompt, temp=1.0, show_prompt=False, show_eos=True):\n        \"\"\"Create a new `LMContext` with a given prompt and temperature.\n\n        Args:\n            lm (hfppl.llms.CachedCausalLM): the language model for which this is a context.\n            prompt (str): a string with which to initialize the context. Will be tokenized using `lm.tokenizer`.\n            temp (float): temeprature for next-token distribution (0 &lt; temp &lt; float('inf'))\n\n        Note:\n            For async initialization of LMContext, use LMContext.create().\n        \"\"\"\n        self._init_common(lm, prompt, temp, show_prompt, show_eos)\n        self.next_token_logprobs = log_softmax(\n            lm.next_token_logprobs_unbatched(self.tokens) / temp\n        )\n\n    @classmethod\n    async def create(cls, lm, prompt, temp=1.0, show_prompt=False, show_eos=True):\n        \"\"\"Asynchronously create a new `LMContext` with a given prompt and temperature.\"\"\"\n        self = cls.__new__(cls)\n        self._init_common(lm, prompt, temp, show_prompt, show_eos)\n        logprobs = await lm.next_token_logprobs(self.tokens)\n        self.next_token_logprobs = log_softmax(logprobs / temp)\n        return self\n\n    def _init_common(self, lm, prompt, temp, show_prompt, show_eos):\n        \"\"\"Initialize common attributes shared between __init__ and create.\"\"\"\n        self.lm = lm\n        self.tokens = lm.tokenizer.encode(prompt)\n        self.temp = temp\n        self.model_mask = lm.masks.ALL_TOKENS\n        self.prompt_string_length = len(lm.tokenizer.decode(self.tokens))\n        self.prompt_token_count = len(self.tokens)\n        self.show_prompt = show_prompt\n        self.show_eos = show_eos\n\n    def next_token(self):\n        \"\"\"Distribution over the next token.\n\n        Sampling or observing from this distribution advances the state of this `LMContext` instance.\n        \"\"\"\n        return LMNextToken(self)\n\n    def mask_dist(self, mask):\n        \"\"\"Bernoulli distribution, with probability of True equal to the probability that the next token of this `LMContext` belongs\n        to the given mask.\n\n        Sampling or observing from this distribution modifies the state of this `LMContext` instance, so that\n        the `next_token()` distribution either *will* (if True) or *will not* (if False) generate a token from\n        the given mask.\n\n        Args:\n            mask: a `set(int)` specifying which token ids are included within the mask.\n        \"\"\"\n        return LMTokenMask(self, mask)\n\n    @property\n    def token_count(self):\n        return len(self.tokens) - self.prompt_token_count\n\n    def __str__(self):\n        full_string = self.lm.tokenizer.decode(self.tokens)\n        if not self.show_prompt:\n            full_string = full_string[self.prompt_string_length :]\n        if not self.show_eos and full_string.endswith(self.lm.tokenizer.eos_token):\n            full_string = full_string[: -len(self.lm.tokenizer.eos_token)]\n        return full_string\n\n    def __deepcopy__(self, memo):\n        cpy = type(self).__new__(type(self))\n\n        for k, v in self.__dict__.items():\n            if k in set([\"lm\"]):\n                setattr(cpy, k, v)\n            else:\n                setattr(cpy, k, copy.deepcopy(v, memo))\n\n        return cpy\n</code></pre>"},{"location":"reference/hfppl/distributions/lmcontext/#hfppl.distributions.lmcontext.LMContext.__init__","title":"<code>__init__(lm, prompt, temp=1.0, show_prompt=False, show_eos=True)</code>","text":"<p>Create a new <code>LMContext</code> with a given prompt and temperature.</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <code>CachedCausalLM</code> <p>the language model for which this is a context.</p> required <code>prompt</code> <code>str</code> <p>a string with which to initialize the context. Will be tokenized using <code>lm.tokenizer</code>.</p> required <code>temp</code> <code>float</code> <p>temeprature for next-token distribution (0 &lt; temp &lt; float('inf'))</p> <code>1.0</code> Note <p>For async initialization of LMContext, use LMContext.create().</p> Source code in <code>hfppl/distributions/lmcontext.py</code> <pre><code>def __init__(self, lm, prompt, temp=1.0, show_prompt=False, show_eos=True):\n    \"\"\"Create a new `LMContext` with a given prompt and temperature.\n\n    Args:\n        lm (hfppl.llms.CachedCausalLM): the language model for which this is a context.\n        prompt (str): a string with which to initialize the context. Will be tokenized using `lm.tokenizer`.\n        temp (float): temeprature for next-token distribution (0 &lt; temp &lt; float('inf'))\n\n    Note:\n        For async initialization of LMContext, use LMContext.create().\n    \"\"\"\n    self._init_common(lm, prompt, temp, show_prompt, show_eos)\n    self.next_token_logprobs = log_softmax(\n        lm.next_token_logprobs_unbatched(self.tokens) / temp\n    )\n</code></pre>"},{"location":"reference/hfppl/distributions/lmcontext/#hfppl.distributions.lmcontext.LMContext.create","title":"<code>create(lm, prompt, temp=1.0, show_prompt=False, show_eos=True)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Asynchronously create a new <code>LMContext</code> with a given prompt and temperature.</p> Source code in <code>hfppl/distributions/lmcontext.py</code> <pre><code>@classmethod\nasync def create(cls, lm, prompt, temp=1.0, show_prompt=False, show_eos=True):\n    \"\"\"Asynchronously create a new `LMContext` with a given prompt and temperature.\"\"\"\n    self = cls.__new__(cls)\n    self._init_common(lm, prompt, temp, show_prompt, show_eos)\n    logprobs = await lm.next_token_logprobs(self.tokens)\n    self.next_token_logprobs = log_softmax(logprobs / temp)\n    return self\n</code></pre>"},{"location":"reference/hfppl/distributions/lmcontext/#hfppl.distributions.lmcontext.LMContext.mask_dist","title":"<code>mask_dist(mask)</code>","text":"<p>Bernoulli distribution, with probability of True equal to the probability that the next token of this <code>LMContext</code> belongs to the given mask.</p> <p>Sampling or observing from this distribution modifies the state of this <code>LMContext</code> instance, so that the <code>next_token()</code> distribution either will (if True) or will not (if False) generate a token from the given mask.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <p>a <code>set(int)</code> specifying which token ids are included within the mask.</p> required Source code in <code>hfppl/distributions/lmcontext.py</code> <pre><code>def mask_dist(self, mask):\n    \"\"\"Bernoulli distribution, with probability of True equal to the probability that the next token of this `LMContext` belongs\n    to the given mask.\n\n    Sampling or observing from this distribution modifies the state of this `LMContext` instance, so that\n    the `next_token()` distribution either *will* (if True) or *will not* (if False) generate a token from\n    the given mask.\n\n    Args:\n        mask: a `set(int)` specifying which token ids are included within the mask.\n    \"\"\"\n    return LMTokenMask(self, mask)\n</code></pre>"},{"location":"reference/hfppl/distributions/lmcontext/#hfppl.distributions.lmcontext.LMContext.next_token","title":"<code>next_token()</code>","text":"<p>Distribution over the next token.</p> <p>Sampling or observing from this distribution advances the state of this <code>LMContext</code> instance.</p> Source code in <code>hfppl/distributions/lmcontext.py</code> <pre><code>def next_token(self):\n    \"\"\"Distribution over the next token.\n\n    Sampling or observing from this distribution advances the state of this `LMContext` instance.\n    \"\"\"\n    return LMNextToken(self)\n</code></pre>"},{"location":"reference/hfppl/distributions/logcategorical/","title":"logcategorical","text":""},{"location":"reference/hfppl/distributions/logcategorical/#hfppl.distributions.logcategorical.LogCategorical","title":"<code>LogCategorical</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>A Geometric distribution.</p> Source code in <code>hfppl/distributions/logcategorical.py</code> <pre><code>class LogCategorical(Distribution):\n    \"\"\"A Geometric distribution.\"\"\"\n\n    def __init__(self, logits):\n        \"\"\"Create a Categorical distribution from unnormalized log probabilities (logits).\n        Given an array of logits, takes their `softmax` and samples an integer in `range(len(logits))`\n        from the resulting categorical.\n\n        Args:\n            logits (np.array): a numpy array of unnormalized log probabilities.\n        \"\"\"\n        self.log_probs = log_softmax(logits)\n\n    async def sample(self):\n        n = np.random.choice(len(self.log_probs), p=np.exp(self.log_probs))\n        return n, await self.log_prob(n)\n\n    async def log_prob(self, value):\n        return self.log_probs[value]\n\n    async def argmax(self, idx):\n        return np.argsort(self.log_probs)[-idx]\n</code></pre>"},{"location":"reference/hfppl/distributions/logcategorical/#hfppl.distributions.logcategorical.LogCategorical.__init__","title":"<code>__init__(logits)</code>","text":"<p>Create a Categorical distribution from unnormalized log probabilities (logits). Given an array of logits, takes their <code>softmax</code> and samples an integer in <code>range(len(logits))</code> from the resulting categorical.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>array</code> <p>a numpy array of unnormalized log probabilities.</p> required Source code in <code>hfppl/distributions/logcategorical.py</code> <pre><code>def __init__(self, logits):\n    \"\"\"Create a Categorical distribution from unnormalized log probabilities (logits).\n    Given an array of logits, takes their `softmax` and samples an integer in `range(len(logits))`\n    from the resulting categorical.\n\n    Args:\n        logits (np.array): a numpy array of unnormalized log probabilities.\n    \"\"\"\n    self.log_probs = log_softmax(logits)\n</code></pre>"},{"location":"reference/hfppl/distributions/tokencategorical/","title":"tokencategorical","text":""},{"location":"reference/hfppl/distributions/tokencategorical/#hfppl.distributions.tokencategorical.TokenCategorical","title":"<code>TokenCategorical</code>","text":"<p>               Bases: <code>Distribution</code></p> Source code in <code>hfppl/distributions/tokencategorical.py</code> <pre><code>class TokenCategorical(Distribution):\n    def __init__(self, lm, logits):\n        \"\"\"Create a Categorical distribution whose values are Tokens, not integers.\n        Given a language model `lm` and an array of unnormalized log probabilities (of length `len(lm.vocab)`),\n        uses softmax to normalize them and samples a Token from the resulting categorical.\n\n        Args:\n            lm (hfppl.llms.CachedCausalLM): the language model whose vocabulary is to be generated from.\n            logits (np.array): a numpy array of unnormalized log probabilities.\n        \"\"\"\n        self.lm = lm\n        self.log_probs = log_softmax(logits)\n        if self.lm.tokenizer.vocab_size != len(logits):\n            raise RuntimeError(\n                f\"TokenCategorical: vocab size is {self.lm.tokenizer.vocab_size} but provided {len(logits)} logits.\"\n            )\n\n    async def sample(self):\n        n = np.random.choice(len(self.log_probs), p=(np.exp(self.log_probs)))\n        return (\n            Token(self.lm, n, self.lm.tokenizer.convert_ids_to_tokens(n)),\n            self.log_probs[n],\n        )\n\n    async def log_prob(self, value):\n        return self.log_probs[value.token_id]\n\n    async def argmax(self, idx):\n        tok = torch.argsort(self.log_probs)[-idx]\n        return (\n            Token(self.lm, tok, self.lm.tokenizer.convert_ids_to_tokens(tok)),\n            self.log_probs[tok],\n        )\n</code></pre>"},{"location":"reference/hfppl/distributions/tokencategorical/#hfppl.distributions.tokencategorical.TokenCategorical.__init__","title":"<code>__init__(lm, logits)</code>","text":"<p>Create a Categorical distribution whose values are Tokens, not integers. Given a language model <code>lm</code> and an array of unnormalized log probabilities (of length <code>len(lm.vocab)</code>), uses softmax to normalize them and samples a Token from the resulting categorical.</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <code>CachedCausalLM</code> <p>the language model whose vocabulary is to be generated from.</p> required <code>logits</code> <code>array</code> <p>a numpy array of unnormalized log probabilities.</p> required Source code in <code>hfppl/distributions/tokencategorical.py</code> <pre><code>def __init__(self, lm, logits):\n    \"\"\"Create a Categorical distribution whose values are Tokens, not integers.\n    Given a language model `lm` and an array of unnormalized log probabilities (of length `len(lm.vocab)`),\n    uses softmax to normalize them and samples a Token from the resulting categorical.\n\n    Args:\n        lm (hfppl.llms.CachedCausalLM): the language model whose vocabulary is to be generated from.\n        logits (np.array): a numpy array of unnormalized log probabilities.\n    \"\"\"\n    self.lm = lm\n    self.log_probs = log_softmax(logits)\n    if self.lm.tokenizer.vocab_size != len(logits):\n        raise RuntimeError(\n            f\"TokenCategorical: vocab size is {self.lm.tokenizer.vocab_size} but provided {len(logits)} logits.\"\n        )\n</code></pre>"},{"location":"reference/hfppl/distributions/transformer/","title":"transformer","text":""},{"location":"reference/hfppl/distributions/transformer/#hfppl.distributions.transformer.Transformer","title":"<code>Transformer</code>","text":"<p>               Bases: <code>Distribution</code></p> Source code in <code>hfppl/distributions/transformer.py</code> <pre><code>class Transformer(Distribution):\n    def __init__(self, lm, prompt, temp=1.0):\n        \"\"\"Create a Categorical distribution whose values are Tokens, with probabilities given\n        by a language model. Supports auto-batching.\n\n        Args:\n            lm (hfppl.llms.CachedCausalLM): the language model.\n            prompt (str | hfppl.llms.TokenSequence): the sequence of tokens to use as the prompt. If a string, `lm.tokenizer` is used to encode it.\n            temp (float): temperature at which to generate (0 &lt; `temp` &lt; `float('inf')`).\n        \"\"\"\n        self.lm = lm\n        self.temp = temp\n\n        # prompt will be a list of ints\n        if isinstance(prompt, str):\n            prompt = self.lm.tokenizer.encode(prompt)\n        elif isinstance(prompt, TokenSequence):\n            prompt = prompt.seq\n\n        self.prompt = prompt\n\n    async def log_prob(self, x):\n        log_probs = await self.lm.next_token_logprobs(self.prompt)\n        log_probs = log_probs / self.temp\n\n        if isinstance(x, Token):\n            x = x.token_id\n\n        return log_probs[x]\n\n    async def sample(self):\n        log_probs = await self.lm.next_token_logprobs(self.prompt)\n        log_probs = log_probs / self.temp\n        probs = np.exp(log_probs)\n        token_id = np.random.choice(len(probs), p=(probs))\n        logprob = log_probs[token_id]\n        return (\n            Token(self.lm, token_id, self.lm.tokenizer.convert_ids_to_tokens(token_id)),\n            logprob,\n        )\n</code></pre>"},{"location":"reference/hfppl/distributions/transformer/#hfppl.distributions.transformer.Transformer.__init__","title":"<code>__init__(lm, prompt, temp=1.0)</code>","text":"<p>Create a Categorical distribution whose values are Tokens, with probabilities given by a language model. Supports auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>lm</code> <code>CachedCausalLM</code> <p>the language model.</p> required <code>prompt</code> <code>str | TokenSequence</code> <p>the sequence of tokens to use as the prompt. If a string, <code>lm.tokenizer</code> is used to encode it.</p> required <code>temp</code> <code>float</code> <p>temperature at which to generate (0 &lt; <code>temp</code> &lt; <code>float('inf')</code>).</p> <code>1.0</code> Source code in <code>hfppl/distributions/transformer.py</code> <pre><code>def __init__(self, lm, prompt, temp=1.0):\n    \"\"\"Create a Categorical distribution whose values are Tokens, with probabilities given\n    by a language model. Supports auto-batching.\n\n    Args:\n        lm (hfppl.llms.CachedCausalLM): the language model.\n        prompt (str | hfppl.llms.TokenSequence): the sequence of tokens to use as the prompt. If a string, `lm.tokenizer` is used to encode it.\n        temp (float): temperature at which to generate (0 &lt; `temp` &lt; `float('inf')`).\n    \"\"\"\n    self.lm = lm\n    self.temp = temp\n\n    # prompt will be a list of ints\n    if isinstance(prompt, str):\n        prompt = self.lm.tokenizer.encode(prompt)\n    elif isinstance(prompt, TokenSequence):\n        prompt = prompt.seq\n\n    self.prompt = prompt\n</code></pre>"},{"location":"reference/hfppl/inference/__init__/","title":"inference","text":"<p>Provides inference methods for use with LLaMPPL models.</p> <p>This module currently provides the following inference methods:</p> <ul> <li> <p><code>smc_standard(model, num_particles, ess_threshold=0.5)</code>: Standard SMC with multinomial resampling.</p> </li> <li> <p><code>smc_steer(model, num_beams, num_expansions)</code>: a without-replacement SMC algorithm that resembles beam search.</p> </li> </ul>"},{"location":"reference/hfppl/inference/smc_record/","title":"smc_record","text":""},{"location":"reference/hfppl/inference/smc_standard/","title":"smc_standard","text":""},{"location":"reference/hfppl/inference/smc_standard/#hfppl.inference.smc_standard.smc_standard","title":"<code>smc_standard(model, n_particles, ess_threshold=0.5, visualization_dir=None, json_file=None)</code>  <code>async</code>","text":"<p>Standard sequential Monte Carlo algorithm with multinomial resampling.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to perform inference on.</p> required <code>n_particles</code> <code>int</code> <p>Number of particles to execute concurrently.</p> required <code>ess_threshold</code> <code>float</code> <p>Effective sample size below which resampling is triggered, given as a fraction of <code>n_particles</code>.</p> <code>0.5</code> <code>visualization_dir</code> <code>str</code> <p>Path to the directory where the visualization server is running.</p> <code>None</code> <code>json_file</code> <code>str</code> <p>Path to the JSON file to save the record of the inference, relative to <code>visualization_dir</code> if provided.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>particles</code> <code>list[Model]</code> <p>The completed particles after inference.</p> Source code in <code>hfppl/inference/smc_standard.py</code> <pre><code>async def smc_standard(\n    model, n_particles, ess_threshold=0.5, visualization_dir=None, json_file=None\n):\n    \"\"\"\n    Standard sequential Monte Carlo algorithm with multinomial resampling.\n\n    Args:\n        model (hfppl.modeling.Model): The model to perform inference on.\n        n_particles (int): Number of particles to execute concurrently.\n        ess_threshold (float): Effective sample size below which resampling is triggered, given as a fraction of `n_particles`.\n        visualization_dir (str): Path to the directory where the visualization server is running.\n        json_file (str): Path to the JSON file to save the record of the inference, relative to `visualization_dir` if provided.\n\n    Returns:\n        particles (list[hfppl.modeling.Model]): The completed particles after inference.\n    \"\"\"\n    particles = [copy.deepcopy(model) for _ in range(n_particles)]\n    await asyncio.gather(*[p.start() for p in particles])\n    record = visualization_dir is not None or json_file is not None\n    history = SMCRecord(n_particles) if record else None\n\n    ancestor_indices = list(range(n_particles))\n    did_resample = False\n    while any(map(lambda p: not p.done_stepping(), particles)):\n        # Step each particle\n        for p in particles:\n            p.untwist()\n        await asyncio.gather(*[p.step() for p in particles if not p.done_stepping()])\n\n        # Record history\n        if record:\n            if len(history.history) == 0:\n                history.add_init(particles)\n            elif did_resample:\n                history.add_resample(ancestor_indices, particles)\n            else:\n                history.add_smc_step(particles)\n\n        # Normalize weights\n        W = np.array([p.weight for p in particles])\n        w_sum = logsumexp(W)\n        normalized_weights = W - w_sum\n\n        # Resample if necessary\n        if -logsumexp(normalized_weights * 2) &lt; np.log(ess_threshold) + np.log(\n            n_particles\n        ):\n            # Alternative implementation uses a multinomial distribution and only makes n-1 copies, reusing existing one, but fine for now\n            probs = np.exp(normalized_weights)\n            ancestor_indices = [\n                np.random.choice(range(len(particles)), p=probs)\n                for _ in range(n_particles)\n            ]\n\n            if record:\n                # Sort the ancestor indices\n                ancestor_indices.sort()\n\n            particles = [copy.deepcopy(particles[i]) for i in ancestor_indices]\n            avg_weight = w_sum - np.log(n_particles)\n            for p in particles:\n                p.weight = avg_weight\n\n            did_resample = True\n        else:\n            did_resample = False\n\n    if record:\n        # Figure out path to save JSON.\n        if visualization_dir is None:\n            json_path = json_file\n        else:\n            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n            json_relative = (\n                json_file\n                if json_file is not None\n                else f\"{model.__class__.__name__}-{timestamp}.json\"\n            )\n            json_path = f\"{visualization_dir}/{json_file}\"\n\n        # Save JSON\n        with open(json_path, \"w\") as f:\n            f.write(history.to_json())\n\n        # Web path is the part of the path after the html directory\n        if visualization_dir is not None:\n            print(f\"Visualize at http://localhost:8000/smc.html?path={json_relative}\")\n        else:\n            print(f\"Saved record to {json_path}\")\n\n    return particles\n</code></pre>"},{"location":"reference/hfppl/inference/smc_steer/","title":"smc_steer","text":""},{"location":"reference/hfppl/inference/smc_steer/#hfppl.inference.smc_steer.smc_steer","title":"<code>smc_steer(model, n_particles, n_beam)</code>  <code>async</code>","text":"<p>Modified sequential Monte Carlo algorithm that uses without-replacement resampling, as described in our workshop abstract.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to perform inference on.</p> required <code>n_particles</code> <code>int</code> <p>Number of particles to maintain.</p> required <code>n_beam</code> <code>int</code> <p>Number of continuations to consider for each particle.</p> required <p>Returns:</p> Name Type Description <code>particles</code> <code>list[Model]</code> <p>The completed particles after inference.</p> Source code in <code>hfppl/inference/smc_steer.py</code> <pre><code>async def smc_steer(model, n_particles, n_beam):\n    \"\"\"\n    Modified sequential Monte Carlo algorithm that uses without-replacement resampling,\n    as described in [our workshop abstract](https://arxiv.org/abs/2306.03081).\n\n    Args:\n        model (hfppl.modeling.Model): The model to perform inference on.\n        n_particles (int): Number of particles to maintain.\n        n_beam (int): Number of continuations to consider for each particle.\n\n    Returns:\n        particles (list[hfppl.modeling.Model]): The completed particles after inference.\n    \"\"\"\n    # Create n_particles copies of the model\n    particles = [copy.deepcopy(model) for _ in range(n_particles)]\n    await asyncio.gather(*[p.start() for p in particles])\n\n    while any(map(lambda p: not p.done_stepping(), particles)):\n        # Count the number of finished particles\n        n_finished = sum(map(lambda p: p.done_stepping(), particles))\n        n_total = n_finished + (n_particles - n_finished) * n_beam\n\n        # Create a super-list of particles that has n_beam copies of each\n        super_particles = []\n        for p in particles:\n            p.untwist()\n            super_particles.append(p)\n            if p.done_stepping():\n                p.weight += np.log(n_total) - np.log(n_particles)\n            else:\n                p.weight += np.log(n_total) - np.log(n_particles) - np.log(n_beam)\n                super_particles.extend([copy.deepcopy(p) for _ in range(n_beam - 1)])\n\n        # Step each super-particle\n        await asyncio.gather(\n            *[p.step() for p in super_particles if not p.done_stepping()]\n        )\n\n        # Use optimal resampling to resample\n        W = np.array([p.weight for p in super_particles])\n        W_tot = logsumexp(W)\n        W_normalized = softmax(W)\n        det_indices, stoch_indices, c = resample_optimal(W_normalized, n_particles)\n        particles = [\n            super_particles[i] for i in np.concatenate((det_indices, stoch_indices))\n        ]\n        # For deterministic particles: w = w * N/N'\n        for i in det_indices:\n            super_particles[i].weight += np.log(n_particles) - np.log(n_total)\n        # For stochastic particles: w = 1/c * total       sum(stoch weights) / num_stoch = sum(stoch weights / total) / num_stoch * total * N/M\n        for i in stoch_indices:\n            super_particles[i].weight = (\n                W_tot - np.log(c) + np.log(n_particles) - np.log(n_total)\n            )\n\n    # Return the particles\n    return particles\n</code></pre>"}]}